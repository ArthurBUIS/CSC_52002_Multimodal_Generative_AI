{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvScrL3XSDUb"
      },
      "source": [
        "# Lab1 - Transformers, ViT and VLM\n",
        "\n",
        "The first two sections are an introduction to Git/GitHub and Pytorch. The last part focuses on Transformers, ViT and VLM. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## I: Introduction to Git and GitHub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook will guide you through the essential commands and operations needed to use Git and GitHub effectively. By the end of this notebook, you will be able to:\n",
        "\n",
        "1. Install and configure Git.\n",
        "2. Create a local repository.\n",
        "3. Make commits to your repository.\n",
        "4. Push your repository to GitHub.\n",
        "5. Collaborate with others using GitHub.\n",
        "\n",
        "### Step 1: Install and Configure Git\n",
        "\n",
        "First, you need to install Git on your computer. You can download it from [git-scm.com](https://git-scm.com/).\n",
        "\n",
        "Once installed, open your terminal (Command Prompt, PowerShell, or Git Bash) and configure Git with your username and email:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (2931567695.py, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    git config --global user.name \"Your Name\"\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "git config --global user.name \"Your Name\"\n",
        "git config --global user.email \"your.email@email.com\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Create a Local Repository\n",
        "\n",
        "1. Create a new directory for your project and navigate into it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "mkdir my_project\n",
        "cd my_project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Initialize a new Git repository:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "git init"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Make Commits to Your Repository\n",
        "\n",
        "1. Create a new file (e.g., README.md) and add some content to it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "echo \"# My Project\" > README.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Check the status of your repository:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "git status"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "3. Add the file to the staging area:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "git add README.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Commit the changes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "git commit -m \"Initial commit\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Push Your Repository to GitHub\n",
        "\n",
        "1. Create a new repository on GitHub. Do not initialize the repository with a README, .gitignore, or license.\n",
        "\n",
        "2. Add the remote repository URL to your local repository:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "git remote add origin https://github.com/yourusername/my_project.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Push your changes to the remote repository:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "git push -u origin master"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Collaborate with Others Using GitHub\n",
        "\n",
        "To collaborate with others, you can add collaborators to your GitHub repository. Go to the repository settings and add collaborators.\n",
        "\n",
        "Collaborators can clone the repository to their local machine:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "git clone https://github.com/yourusername/my_project.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Collaborators can create a new branch to work on a feature (A simple rule is: \"one new feature, one branch, one pull request\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "git checkout -b feature-branch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "They can make changes and commit them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "git add .\n",
        "git commit -m \"Add new feature\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Push the branch to the remote repository:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "git push origin feature-branch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a pull request on GitHub to merge the branch into the main branch.\n",
        "\n",
        "1. Click on \"Contribute\" and \"Open pull request\".\n",
        "\n",
        "2. Select the Base (the branch you want to merge your changes into, typically the `main` or `master` branch.) and Compare (the branch that contains your changes) ranches:\n",
        "\n",
        "3. Review the Changes: GitHub will show you the changes between the two branches. Review these changes to ensure everything looks correct.\n",
        "\n",
        "4. Fill out the pull request form and create the pull request:\n",
        "\n",
        "5. Review and Merge:\n",
        "\n",
        "The pull request will now be visible to collaborators, who can review your changes, leave comments, and request modifications. If reviewers request changes, make the necessary modifications in your branch, commit them, and push the updates to the same branch. The pull request will automatically update with the new changes. Once the pull request is approved, you or a collaborator with merge permissions can merge the pull request into the base branch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Additional Git Commands\n",
        "\n",
        "- Check the status of your repository:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "git status"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- View the commit history:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "git log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Switch to a different branch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "git checkout branch-name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Merge a branch into the current branch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "git merge branch-name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Fetch updates from the remote repository:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "git fetch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Pull updates from the remote repository and merge them into the current branch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "git pull"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## II: Introduction to PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyTorch is an open-source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing, primarily as a research platform that provides dynamic computation graphs and rich ecosystems of tools and libraries.\n",
        "\n",
        "In this section, we will:\n",
        "1. Introduce PyTorch and its basic concepts.\n",
        "2. Learn how to manipulate tensors.\n",
        "3. Define a simple Multi-Layer Perceptron (MLP).\n",
        "4. Train the MLP on a simple dataset.\n",
        "\n",
        "First, let's download and import PyTorch and check its version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: torch in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (2.7.1+cu118)\n",
            "Requirement already satisfied: torchvision in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (0.22.1+cu118)\n",
            "Requirement already satisfied: torchaudio in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (2.7.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/lib64/python3.9/site-packages (1.23.5)\n",
            "Requirement already satisfied: networkx in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch) (11.8.87)\n",
            "Requirement already satisfied: jinja2 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch) (11.8.86)\n",
            "Requirement already satisfied: triton==3.3.1 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch) (3.3.1)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: filelock in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: fsspec in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch) (2025.9.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3.9/site-packages (from triton==3.3.1->torch) (53.0.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib64/python3.9/site-packages (from torchvision) (10.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from jinja2->torch) (2.1.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.7.1+cu118\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Tensors\n",
        "\n",
        "Tensors are the core data structure in PyTorch. They are similar to NumPy arrays but with added functionality for GPU acceleration and automatic differentiation. Tensors are optimized for automatic differentiation (we will see more about that later in the Gradient section).\n",
        "\n",
        "A. Creating Tensors\n",
        "\n",
        "Tensor can be created \n",
        "- directly from data. The data type is automatically inferred. \n",
        "- from NumPy arrays\n",
        "- from another tensor\n",
        "- with random or constant values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor from list: \n",
            " tensor([1, 2, 3, 4, 5])\n",
            "Ones Tensor: \n",
            " tensor([1, 2, 3, 4, 5]) \n",
            "\n",
            "Random Tensor: \n",
            " tensor([0.2824, 0.6258, 0.9962, 0.6457, 0.5824]) \n",
            "\n",
            "Tensor with random values: \n",
            " tensor([[0.4488, 0.7791, 0.5488],\n",
            "        [0.4749, 0.7091, 0.7459],\n",
            "        [0.1148, 0.3517, 0.6834]]) \n",
            "\n",
            "Tensor with zeros:\n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]]) \n",
            "\n",
            "Tensor with ones: \n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Creating a tensor from a list\n",
        "x_from_list = torch.tensor([1, 2, 3, 4, 5])\n",
        "print(f\"Tensor from list: \\n {x_from_list}\")\n",
        "\n",
        "# Creating a tensor from a numpy array\n",
        "import numpy as np\n",
        "numpy_array = np.array([1, 2, 3, 4, 5])\n",
        "x_from_numpy = torch.tensor(numpy_array)\n",
        "\n",
        "\n",
        "# Creating a tensor from another tensor\n",
        "x_ones = torch.ones_like(x_from_list)\n",
        "print(f\"Ones Tensor: \\n {x_from_list} \\n\")\n",
        "\n",
        "x_rand = torch.rand_like(x_from_list, dtype=torch.float)\n",
        "print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
        "\n",
        "\n",
        "# Creating a tensor with random values\n",
        "x_random = torch.rand((3, 3))\n",
        "print(f\"Tensor with random values: \\n {x_random} \\n\")\n",
        "\n",
        "\n",
        "# Creating a tensor with constant values\n",
        "x_zeros = torch.zeros((3, 3))\n",
        "print(f\"Tensor with zeros:\\n {x_zeros} \\n\")\n",
        "\n",
        "x_ones = torch.ones((3, 3))\n",
        "print(f\"Tensor with ones: \\n {x_ones} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tensor attributes describe their shape, datatype, and the device on which they are stored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of tensor: torch.Size([3, 4])\n",
            "Datatype of tensor: torch.float32\n",
            "Device tensor is stored on: cpu\n"
          ]
        }
      ],
      "source": [
        "tensor = torch.rand(3,4)\n",
        "\n",
        "print(f\"Shape of tensor: {tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "print(f\"Device tensor is stored on: {tensor.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We notice that the variable `tensor` has 'cpu' as device attribute. PyTorch allows you to store tensors and perform computations on different devices, such as the CPU and GPU. Using a GPU can significantly speed up training and inference for large models and datasets.\n",
        "\n",
        "By default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using .to method. First, let's check if a GPU is available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you’re using Colab, you can allocate a GPU by going to Runtime > Change runtime type > GPU. \n",
        "You can move tensors to a specific device using the .to(device) method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor on device: cpu\n",
            "Tensor on device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Create a tensor (by default on the cpu)\n",
        "tensor_cpu = torch.tensor([1.0, 2.0, 3.0])\n",
        "print(f\"Tensor on device: {tensor_cpu.device}\")\n",
        "\n",
        "# Move the tensor to the selected device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    tensor_device = tensor_cpu.to(device)\n",
        "    print(f\"Tensor on device: {tensor_device.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Tensor operations\n",
        "\n",
        "Lots of tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing), sampling and more are already implemented in PyTorch. The next cells show some basic operations. You can find comprehensively list of available operations [here](https://pytorch.org/docs/stable/torch.html).\n",
        "\n",
        "Arithmetic operation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sum of tensors: \n",
            " tensor([5, 7, 9])\n",
            "\n",
            "Product of tensors: \n",
            " tensor([ 4, 10, 18]) \n",
            "\n",
            "Matrix multiplication: \n",
            " tensor([[19, 22],\n",
            "        [43, 50]]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Adding two tensors\n",
        "tensor_a = torch.tensor([1, 2, 3])\n",
        "tensor_b = torch.tensor([4, 5, 6])\n",
        "tensor_sum = tensor_a + tensor_b\n",
        "print(f\"Sum of tensors: \\n {tensor_sum}\\n\")\n",
        "\n",
        "\n",
        "# Multiplying two tensors element-wise. All three result tensors will have the same value\n",
        "tensor_product_1 = tensor_a * tensor_b\n",
        "tensor_product_2 = tensor_a.mul(tensor_b)\n",
        "tensor_product_3 = torch.mul(tensor_a, tensor_b)\n",
        "print(f\"Product of tensors: \\n {tensor_product_1} \\n\")\n",
        "\n",
        "y1 = tensor @ tensor.T\n",
        "y2 = tensor.matmul(tensor.T)\n",
        "\n",
        "y3 = torch.rand_like(y1)\n",
        "torch.matmul(tensor, tensor.T, out=y3)\n",
        "\n",
        "\n",
        "# Matrix multiplication. All three result tensors will have the same value\n",
        "tensor_c = torch.tensor([[1, 2], [3, 4]])\n",
        "tensor_d = torch.tensor([[5, 6], [7, 8]])\n",
        "tensor_matmul_1 = tensor_c.matmul(tensor_d)\n",
        "tensor_matmul_2 = tensor_c @ tensor_d\n",
        "tensor_matmul_3 = torch.matmul(tensor_c, tensor_d)\n",
        "print(f\"Matrix multiplication: \\n {tensor_matmul_1} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Indexing and Slicing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2D Tensor: \n",
            " tensor([[1, 2, 3],\n",
            "        [4, 5, 6],\n",
            "        [7, 8, 9]]) \n",
            "\n",
            "Element at (0, 0): \n",
            " 1 \n",
            "\n",
            "First row: \n",
            " tensor([1, 2, 3]) \n",
            "\n",
            "First column: \n",
            " tensor([1, 4, 7]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Creating a 2D tensor\n",
        "tensor_2d = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "print(f\"2D Tensor: \\n {tensor_2d} \\n\")\n",
        "\n",
        "# Indexing\n",
        "print(f\"Element at (0, 0): \\n {tensor_2d[0, 0]} \\n\")\n",
        "\n",
        "# Slicing\n",
        "print(f\"First row: \\n {tensor_2d[0, :]} \\n\")\n",
        "print(f\"First column: \\n {tensor_2d[:, 0]} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Single tensor operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean of tensor: \n",
            " 3.0 - <class 'torch.Tensor'> \n",
            "\n",
            "Mean of tensor (item): \n",
            " 3.0 - <class 'float'>\n",
            "\n",
            "Sum of tensor: \n",
            " 15.0 \n",
            "\n",
            "Maximum value in tensor: \n",
            " 5.0 \n",
            "\n",
            "Minimum value in tensor: \n",
            " 1.0 \n",
            "\n",
            "Standard deviation of tensor: \n",
            " 1.5811388492584229 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "tensor = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])\n",
        "\n",
        "## Statistical operations: \n",
        "mean_value = tensor.mean()\n",
        "print(f\"Mean of tensor: \\n {mean_value} - {type(mean_value)} \\n\")\n",
        "mean_value_item = mean_value.item()\n",
        "print(f\"Mean of tensor (item): \\n {mean_value_item} - {type(mean_value_item)}\\n\")\n",
        "\n",
        "# Sum of all elements\n",
        "sum_value = tensor.sum()\n",
        "print(f\"Sum of tensor: \\n {sum_value} \\n\")\n",
        "\n",
        "# Maximum value\n",
        "max_value = tensor.max()\n",
        "print(f\"Maximum value in tensor: \\n {max_value} \\n\")\n",
        "\n",
        "# Minimum value\n",
        "min_value = tensor.min()\n",
        "print(f\"Minimum value in tensor: \\n {min_value} \\n\")\n",
        "\n",
        "# Standard deviation\n",
        "std_value = tensor.std()\n",
        "print(f\"Standard deviation of tensor: \\n {std_value} \\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Concatenation and operation on the dimension "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial tensor: \n",
            " tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]]) \n",
            "\n",
            "Concatenated tensor (dim=1): \n",
            " tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]) \n",
            "\n",
            "Concatenated tensor (dim=0): \n",
            " tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]]) \n",
            "\n",
            "Tensor with ones shape: \n",
            " torch.Size([3, 1]) \n",
            "\n",
            "Squeezed tensor shape: \n",
            " torch.Size([3]) \n",
            "\n",
            "Unsqueezed tensor shape at dimension 0 (shape): \n",
            " torch.Size([1, 4, 4]) \n",
            "\n",
            "Unsqueezed tensor at dimension 1 (shape): \n",
            " torch.Size([4, 1, 4]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "tensor = torch.ones(4, 4)\n",
        "print(f\"Initial tensor: \\n {tensor} \\n\")\n",
        "\n",
        "# Concatenation of tensors\n",
        "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
        "print(f\"Concatenated tensor (dim=1): \\n {t1} \\n\")\n",
        "\n",
        "t2 = torch.cat([tensor, tensor, tensor], dim=0)\n",
        "print(f\"Concatenated tensor (dim=0): \\n {t2} \\n\")\n",
        "\n",
        "# Squeeze and Unsqueeze operations\n",
        "tensor_with_ones = torch.tensor([[1.0], [2.0], [3.0]])\n",
        "print(f\"Tensor with ones shape: \\n {tensor_with_ones.shape} \\n\")\n",
        "squeezed_tensor = tensor_with_ones.squeeze()\n",
        "print(f\"Squeezed tensor shape: \\n {squeezed_tensor.shape} \\n\")\n",
        "\n",
        "unsqueezed_tensor = tensor.unsqueeze(0)\n",
        "print(f\"Unsqueezed tensor shape at dimension 0 (shape): \\n {unsqueezed_tensor.shape} \\n\")\n",
        "\n",
        "# Add a dimension of size 1 at position 1\n",
        "unsqueezed_tensor_1 = tensor.unsqueeze(1)\n",
        "print(f\"Unsqueezed tensor at dimension 1 (shape): \\n {unsqueezed_tensor_1.shape} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Neural networks\n",
        "\n",
        "Neural networks comprise of layers/modules that perform operations on data. Neural networks in PyTorch subclasses the nn.Module. A neural network is a module itself that consists of other modules (layers). \n",
        "\n",
        "To create a neural network in PyTorch, follow these steps:\n",
        "1. Define a class that inherits from `nn.Module`.\n",
        "2. Implement the `__init__` method to initialize the layers of the network.\n",
        "3. Implement the `forward` method to define the forward pass of the network.\n",
        "\n",
        "\n",
        "The `__init__` function is used to initialize the layers of the neural network. In this example, we will define an MLP with one hidden layer.\n",
        "\n",
        "The `forward` takes the input data x and passes it through the first fully connected layer (fc1). This layer applies a linear transformation to the input data. The first layer is then passed through the ReLU activation function. he output of the ReLU activation is finally passed through the second fully connected layer (fc2).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)  # First fully connected layer\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)  # Second fully connected layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))  # Apply ReLU activation to the first layer\n",
        "        x = self.fc2(x)  # Apply the second layer\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our MLP class is now defined. We can create an instance this class and move it to the `device`: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SimpleMLP(\n",
            "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Define the MLP\n",
        "input_size = 784\n",
        "hidden_size = 256\n",
        "output_size = 10\n",
        "model = SimpleMLP(input_size, hidden_size, output_size)\n",
        "model.to(device)\n",
        "\n",
        "# Print the model architecture\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To use the model, we pass it the input data. This executes the model’s forward, along with some background operations. Do not call `model.forward()` directly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted class: 8\n"
          ]
        }
      ],
      "source": [
        "X = torch.rand(1, 784, device=device)\n",
        "logits = model(X)\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s break down the layers in the model.\n",
        "We consider an input tensor `x` of shape `[1, 784]` (1 for the batch size, 784 for the size).\n",
        "\n",
        "- nn.Linear\n",
        "\n",
        "The linear layer is a module that applies a linear transformation on the input using its stored weights and biases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 20])\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(1, 784, device=device)\n",
        "\n",
        "layer1 = nn.Linear(in_features=784, out_features=20, device=device)\n",
        "hidden1 = layer1(x)\n",
        "print(hidden1.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- nn.ReLU\n",
        "\n",
        "Non-linear activations are what create the complex mappings between the model’s inputs and outputs. They are applied after linear transformations to introduce nonlinearity, helping neural networks learn a wide variety of phenomena.\n",
        "\n",
        "In this model, we use nn.ReLU between our linear layers. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before ReLU: tensor([[-0.1773, -0.2566, -0.0387,  0.1032,  0.9115,  0.7156, -0.3300, -0.1091,\n",
            "         -0.3234,  0.1020, -0.1095,  0.8357, -0.1295,  0.1232, -0.0343,  0.0808,\n",
            "         -0.2553,  0.2188,  0.0898, -0.1184]], device='cuda:0',\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "\n",
            "\n",
            "After ReLU: tensor([[0.0000, 0.0000, 0.0000, 0.1032, 0.9115, 0.7156, 0.0000, 0.0000, 0.0000,\n",
            "         0.1020, 0.0000, 0.8357, 0.0000, 0.1232, 0.0000, 0.0808, 0.0000, 0.2188,\n",
            "         0.0898, 0.0000]], device='cuda:0', grad_fn=<ReluBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
        "hidden1 = nn.ReLU()(hidden1)\n",
        "print(f\"After ReLU: {hidden1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- nn.Sequential\n",
        "\n",
        "nn.Sequential is an ordered container of modules. The data is passed through all the modules in the same order as defined. You can use sequential containers to put together a quick network like seq_modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seq_modules = nn.Sequential(\n",
        "    layer1,\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 10, device=device)\n",
        ")\n",
        "logits = seq_modules(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- nn.Softmax\n",
        "\n",
        "The last linear layer of the neural network returns logits - raw values in [-infty, infty] - which are passed to the nn.Softmax module. The logits are scaled to values [0, 1] representing the model’s predicted probabilities for each class. dim parameter indicates the dimension along which the values must sum to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted probabilities: \n",
            " tensor([[0.1136, 0.1254, 0.0884, 0.1181, 0.0896, 0.0737, 0.0898, 0.1132, 0.0771,\n",
            "         0.1112]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)\n",
        "print(f\"Predicted probabilities: \\n {pred_probab} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Many layers inside a neural network are parameterized, i.e. have associated weights and biases that are optimized during training. You can access to the parameters of a model using: `parameters()` or `named_parameters()` methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model structure: SimpleMLP(\n",
            "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n",
            "\n",
            "\n",
            "Layer: fc1.weight | Size: torch.Size([256, 784]) \n",
            "\n",
            "Layer: fc1.bias | Size: torch.Size([256]) \n",
            "\n",
            "Layer: fc2.weight | Size: torch.Size([10, 256]) \n",
            "\n",
            "Layer: fc2.bias | Size: torch.Size([10]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"Model structure: {model}\\n\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Differentiation and gradients in PyTorch\n",
        "\n",
        "When training neural networks, the most frequently used algorithm is back propagation. In this algorithm, parameters (model weights) are adjusted according to the gradient of the loss function with respect to the given parameter.\n",
        "\n",
        "To compute those gradients, PyTorch has a built-in differentiation engine called torch.autograd. It supports automatic computation of gradient for any computational graph.\n",
        "\n",
        "To compute the derivative (of a variable, a loss, etc. with respect to a parameters), we use the `.backward()` method:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient of z with respect to x: tensor([2., 4., 6.])\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "\n",
        "y = x ** 2\n",
        "z = y.sum()\n",
        "\n",
        "# Compute the gradient\n",
        "z.backward()\n",
        "\n",
        "print(\"Gradient of z with respect to x:\", x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consider the simplest one-layer neural network `z = w x + b`, with input x, parameters w and b, and some loss function.\n",
        "\n",
        "In this network, w and b are parameters, which we need to optimize. Thus, we need to be able to compute the gradients of loss function with respect to those variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient of the loss with respect to w: \n",
            " tensor([[0.2683, 0.2235, 0.2249],\n",
            "        [0.2683, 0.2235, 0.2249],\n",
            "        [0.2683, 0.2235, 0.2249],\n",
            "        [0.2683, 0.2235, 0.2249],\n",
            "        [0.2683, 0.2235, 0.2249]]) \n",
            "\n",
            "Gradient of the loss with respect to b: \n",
            " tensor([0.2683, 0.2235, 0.2249]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "x = torch.ones(5)  # input tensor\n",
        "y = torch.zeros(3)  # expected output\n",
        "w = torch.randn(5, 3, requires_grad=True)\n",
        "b = torch.randn(3, requires_grad=True)\n",
        "z = torch.matmul(x, w)+b\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
        "\n",
        "loss.backward()\n",
        "print(f\"Gradient of the loss with respect to w: \\n {w.grad} \\n\")\n",
        "print(f\"Gradient of the loss with respect to b: \\n {b.grad} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A function that we apply to tensors to construct computational graph is in fact an object of class Function. This object knows how to compute the function in the forward direction, and also how to compute its derivative during the backward propagation step. A reference to the backward propagation function is stored in grad_fn property of a tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient function for z = <AddBackward0 object at 0x7fa2201c8dc0>\n",
            "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7fa2201c87f0>\n"
          ]
        }
      ],
      "source": [
        "print(f\"Gradient function for z = {z.grad_fn}\")\n",
        "print(f\"Gradient function for loss = {loss.grad_fn}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Training a neural network\n",
        "\n",
        "Training a neural network in PyTorch involves several key steps: defining the model, preparing the data, specifying the loss function and optimizer, and iterating through the training loop. Let's go through each step in detail.\n",
        "\n",
        "1. Define the model \n",
        "\n",
        "We will use the Multi-Layer Perceptron defined in the previous section.\n",
        "\n",
        "2. Prepare the data\n",
        "\n",
        "Next, we need to prepare our training and evaluation data. PyTorch provides `Dataset` and `DataLoader` classes to handle data loading and preprocessing efficiently. The `Dataset` class is an abstract class representing a dataset, and the `DataLoader` class provides an iterable over the given dataset. The `torchvision.datasets` module contains Dataset objects for many real-world vision data like CIFAR or COCO. \n",
        "\n",
        "In this example, we will use the [MNIST](https://yann.lecun.com/exdb/mnist/) dataset. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyTorch provides `Dataset` and `DataLoader` classes to handle data loading and preprocessing efficiently. The `Dataset` class is an abstract class representing a dataset, and the `DataLoader` class provides an iterable over the given dataset.\n",
        "\n",
        "In this section, we will work with the MNIST Dataset. The `torchvision.datasets` module contains Dataset objects for many real-world vision data like CIFAR or COCO. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 15.3MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 370kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.52MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.03MB/s]\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets\n",
        "import torchvision.transforms as T\n",
        "\n",
        "training_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=T.Compose([T.ToTensor(), T.Lambda(lambda x: torch.flatten(x))]),\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=T.Compose([T.ToTensor(), T.Lambda(lambda x: torch.flatten(x))]),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then pass the Dataset as an argument to DataLoader. The DataLoader class supports batching, shuffling, and loading data in parallel using multiprocessing workers. Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 784])\n",
            "Shape of y: torch.Size([64]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Specify the Loss Function and Optimizer\n",
        "\n",
        "The loss function measures the difference between the predicted outputs and the true labels. The optimizer updates the model parameters to minimize the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Training Loop\n",
        "\n",
        "The training loop involves iterating over the dataset, performing forward and backward passes, and updating the model parameters. The main steps are: \n",
        "- Zero the Parameter Gradients: `optimizer.zero_grad()` clears the old gradients from the last step. This is important because PyTorch accumulates gradients by default.\n",
        "- Forward Pass: The input data is passed through the model to get the outputs. The loss is then computed using the criterion.\n",
        "- Backward Pass: `loss.backward()` computes the gradient of the loss with respect to the model parameters. \n",
        "- Optimization: `optimizer.step()` updates the model parameters using the computed gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Step [100/938], Loss: 1.3691\n",
            "Epoch [1/10], Step [200/938], Loss: 0.5251\n",
            "Epoch [1/10], Step [300/938], Loss: 0.4379\n",
            "Epoch [1/10], Step [400/938], Loss: 0.3567\n",
            "Epoch [1/10], Step [500/938], Loss: 0.3650\n",
            "Epoch [1/10], Step [600/938], Loss: 0.3255\n",
            "Epoch [1/10], Step [700/938], Loss: 0.3210\n",
            "Epoch [1/10], Step [800/938], Loss: 0.3181\n",
            "Epoch [1/10], Step [900/938], Loss: 0.2627\n",
            "Epoch [2/10], Step [100/938], Loss: 0.2338\n",
            "Epoch [2/10], Step [200/938], Loss: 0.2686\n",
            "Epoch [2/10], Step [300/938], Loss: 0.2459\n",
            "Epoch [2/10], Step [400/938], Loss: 0.2224\n",
            "Epoch [2/10], Step [500/938], Loss: 0.2367\n",
            "Epoch [2/10], Step [600/938], Loss: 0.2130\n",
            "Epoch [2/10], Step [700/938], Loss: 0.2176\n",
            "Epoch [2/10], Step [800/938], Loss: 0.2245\n",
            "Epoch [2/10], Step [900/938], Loss: 0.1812\n",
            "Epoch [3/10], Step [100/938], Loss: 0.1627\n",
            "Epoch [3/10], Step [200/938], Loss: 0.1959\n",
            "Epoch [3/10], Step [300/938], Loss: 0.1740\n",
            "Epoch [3/10], Step [400/938], Loss: 0.1622\n",
            "Epoch [3/10], Step [500/938], Loss: 0.1765\n",
            "Epoch [3/10], Step [600/938], Loss: 0.1595\n",
            "Epoch [3/10], Step [700/938], Loss: 0.1627\n",
            "Epoch [3/10], Step [800/938], Loss: 0.1724\n",
            "Epoch [3/10], Step [900/938], Loss: 0.1366\n",
            "Epoch [4/10], Step [100/938], Loss: 0.1258\n",
            "Epoch [4/10], Step [200/938], Loss: 0.1528\n",
            "Epoch [4/10], Step [300/938], Loss: 0.1326\n",
            "Epoch [4/10], Step [400/938], Loss: 0.1270\n",
            "Epoch [4/10], Step [500/938], Loss: 0.1401\n",
            "Epoch [4/10], Step [600/938], Loss: 0.1278\n",
            "Epoch [4/10], Step [700/938], Loss: 0.1294\n",
            "Epoch [4/10], Step [800/938], Loss: 0.1399\n",
            "Epoch [4/10], Step [900/938], Loss: 0.1095\n",
            "Epoch [5/10], Step [100/938], Loss: 0.1030\n",
            "Epoch [5/10], Step [200/938], Loss: 0.1244\n",
            "Epoch [5/10], Step [300/938], Loss: 0.1069\n",
            "Epoch [5/10], Step [400/938], Loss: 0.1045\n",
            "Epoch [5/10], Step [500/938], Loss: 0.1159\n",
            "Epoch [5/10], Step [600/938], Loss: 0.1065\n",
            "Epoch [5/10], Step [700/938], Loss: 0.1080\n",
            "Epoch [5/10], Step [800/938], Loss: 0.1167\n",
            "Epoch [5/10], Step [900/938], Loss: 0.0915\n",
            "Epoch [6/10], Step [100/938], Loss: 0.0873\n",
            "Epoch [6/10], Step [200/938], Loss: 0.1042\n",
            "Epoch [6/10], Step [300/938], Loss: 0.0897\n",
            "Epoch [6/10], Step [400/938], Loss: 0.0888\n",
            "Epoch [6/10], Step [500/938], Loss: 0.0986\n",
            "Epoch [6/10], Step [600/938], Loss: 0.0912\n",
            "Epoch [6/10], Step [700/938], Loss: 0.0930\n",
            "Epoch [6/10], Step [800/938], Loss: 0.1000\n",
            "Epoch [6/10], Step [900/938], Loss: 0.0782\n",
            "Epoch [7/10], Step [100/938], Loss: 0.0757\n",
            "Epoch [7/10], Step [200/938], Loss: 0.0888\n",
            "Epoch [7/10], Step [300/938], Loss: 0.0771\n",
            "Epoch [7/10], Step [400/938], Loss: 0.0775\n",
            "Epoch [7/10], Step [500/938], Loss: 0.0858\n",
            "Epoch [7/10], Step [600/938], Loss: 0.0796\n",
            "Epoch [7/10], Step [700/938], Loss: 0.0813\n",
            "Epoch [7/10], Step [800/938], Loss: 0.0872\n",
            "Epoch [7/10], Step [900/938], Loss: 0.0684\n",
            "Epoch [8/10], Step [100/938], Loss: 0.0665\n",
            "Epoch [8/10], Step [200/938], Loss: 0.0770\n",
            "Epoch [8/10], Step [300/938], Loss: 0.0674\n",
            "Epoch [8/10], Step [400/938], Loss: 0.0683\n",
            "Epoch [8/10], Step [500/938], Loss: 0.0760\n",
            "Epoch [8/10], Step [600/938], Loss: 0.0705\n",
            "Epoch [8/10], Step [700/938], Loss: 0.0718\n",
            "Epoch [8/10], Step [800/938], Loss: 0.0769\n",
            "Epoch [8/10], Step [900/938], Loss: 0.0605\n",
            "Epoch [9/10], Step [100/938], Loss: 0.0590\n",
            "Epoch [9/10], Step [200/938], Loss: 0.0676\n",
            "Epoch [9/10], Step [300/938], Loss: 0.0594\n",
            "Epoch [9/10], Step [400/938], Loss: 0.0608\n",
            "Epoch [9/10], Step [500/938], Loss: 0.0683\n",
            "Epoch [9/10], Step [600/938], Loss: 0.0630\n",
            "Epoch [9/10], Step [700/938], Loss: 0.0640\n",
            "Epoch [9/10], Step [800/938], Loss: 0.0685\n",
            "Epoch [9/10], Step [900/938], Loss: 0.0541\n",
            "Epoch [10/10], Step [100/938], Loss: 0.0528\n",
            "Epoch [10/10], Step [200/938], Loss: 0.0597\n",
            "Epoch [10/10], Step [300/938], Loss: 0.0527\n",
            "Epoch [10/10], Step [400/938], Loss: 0.0547\n",
            "Epoch [10/10], Step [500/938], Loss: 0.0618\n",
            "Epoch [10/10], Step [600/938], Loss: 0.0565\n",
            "Epoch [10/10], Step [700/938], Loss: 0.0575\n",
            "Epoch [10/10], Step [800/938], Loss: 0.0614\n",
            "Epoch [10/10], Step [900/938], Loss: 0.0486\n",
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_dataloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:  # Print every 100 mini-batches\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader)}], Loss: {running_loss/100:.4f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print(\"Training complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Evaluation\n",
        "\n",
        "After training, you can evaluate the model on the test dataset to see how well it performs. We need to:\n",
        "- Set the Model to Evaluation Mode: `model.eval()` sets the model to evaluation model\n",
        "- Disable Gradient Calculation: `torch.no_grad()` disables gradient calculation. It also reduce memory consumption and speeds up computation.\n",
        "- Compute Accuracy: The model's predictions are compared to the true labels to compute the accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 97.78%\n"
          ]
        }
      ],
      "source": [
        "# Evaluation loop\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():  # Disable gradient calculation\n",
        "    for data in test_dataloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## III: Attention and Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this part, we will implement the Transformers architecture. Transformers has been a key architecture in deep learning for the past 5 years. \n",
        "\n",
        "It has first began with NLP, then came audio and finally, since 2020, computer vision.\n",
        "We will implement every block that makes a transformer from scratch and we will try to create a deep understanding of what is happening.\n",
        "Here is a figure for the transformer architecture:\n",
        "\n",
        "\n",
        "<img src=\"https://www.researchgate.net/profile/Miruna-Gheata/publication/355339249/figure/fig1/AS:1079476452622337@1634378650979/Encoder-decoder-architecture-of-the-Transformer-developed-by-Vaswani-et-al-28.ppm\" width=768>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: einops in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (0.8.1)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: timm in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (1.0.24)\n",
            "Requirement already satisfied: pyyaml in /usr/lib64/python3.9/site-packages (from timm) (5.4.1)\n",
            "Requirement already satisfied: torch in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from timm) (2.7.1+cu118)\n",
            "Requirement already satisfied: safetensors in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from timm) (0.7.0)\n",
            "Requirement already satisfied: huggingface_hub in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from timm) (0.36.0)\n",
            "Requirement already satisfied: torchvision in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from timm) (0.22.1+cu118)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from huggingface_hub->timm) (4.15.0)\n",
            "Requirement already satisfied: requests in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from huggingface_hub->timm) (2.32.5)\n",
            "Requirement already satisfied: packaging>=20.9 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from huggingface_hub->timm) (25.0)\n",
            "Requirement already satisfied: filelock in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from huggingface_hub->timm) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from huggingface_hub->timm) (2025.9.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from huggingface_hub->timm) (1.2.0)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch->timm) (11.8.86)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch->timm) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch->timm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch->timm) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch->timm) (10.3.0.86)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch->timm) (1.14.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch->timm) (11.8.89)\n",
            "Requirement already satisfied: jinja2 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch->timm) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch->timm) (11.8.89)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch->timm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch->timm) (11.4.1.48)\n",
            "Requirement already satisfied: triton==3.3.1 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch->timm) (3.3.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch->timm) (11.7.5.86)\n",
            "Requirement already satisfied: networkx in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from torch->timm) (3.2.1)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3.9/site-packages (from triton==3.3.1->torch->timm) (53.0.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib64/python3.9/site-packages (from torchvision->timm) (10.0.1)\n",
            "Requirement already satisfied: numpy in /usr/lib64/python3.9/site-packages (from torchvision->timm) (1.23.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from requests->huggingface_hub->timm) (2.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from requests->huggingface_hub->timm) (3.4.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from requests->huggingface_hub->timm) (2025.11.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /users/eleves-b/2023/arthur.buis/.local/lib/python3.9/site-packages (from requests->huggingface_hub->timm) (3.11)\n"
          ]
        }
      ],
      "source": [
        "!pip install einops\n",
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "import math\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "# Fix matplotlib version issue\n",
        "import matplotlib\n",
        "matplotlib.__version_info__ = tuple(map(int, matplotlib.__version__.split('.')[:2]))\n",
        "matplotlib.use('Agg')  # Non-interactive backend\n",
        "import numpy as np\n",
        "from einops import rearrange, repeat\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import requests\n",
        "\n",
        "torch.manual_seed(3407)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### III-1. The attention mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementing the scaled dot-product attention mechanism\n",
        "\n",
        "The transformer architecture is built around one key block: The attention.\n",
        "The idea behind attention is the following. Imagine you want to retrieve information from a dictionary. The dictionnary is indexed by keys which maps to a particular value. Now, you have a query which will be matched against the keys of the dict and if you have a match, you will retrieve the associated value.\n",
        "Attention is very similar to this simple retrieval example. Now, with real data, we don't have this structure, we however are going to learn to create it. \n",
        "\n",
        "We have 2 sets of vectors (also named tokens). One is $X_{to}$ which is the destination set. We want to be able to map this set of tokens to queries. We achieve this by doing a linear projection of $X_{to}$ to obatain:  $Q = W_QX_{to}$\n",
        "\n",
        "The other set is $X_{from}$ the set from which we want to retrieve information. We will need to extract both keys and values from this set. We therefore do 2 linear projections of $X_{from}$ to obtain:  $K = W_KX_{from}$ and $V = W_VX_{from}$.\n",
        "\n",
        "Now, contrary to the dictionnary where queries and values are exact matchs, we don't have this here. Therefore, we will perform a softer match by computing the similarity matrix between $Q$ and $K$. Then for each $Q$, we want to output the values that have the higher similarity. We therefore output the weighted sum of the values, weighted by the softmax of the similarity (also called the attention matrix).\n",
        "\n",
        "Finally, the attention operation is given by the cross attention:\n",
        "\n",
        "$$\n",
        "A(Q,K,V) = \\text{SoftMax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
        "$$\n",
        "\n",
        "We divide the similarity by $\\sqrt{d_k}$ for stability reason to avoid the similarity to explode with big vectors which would lead to very sharp attention coeficients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Question 1.\n",
        "Implement the attention operation, use  `torch.einsum` to easily compute the similarity matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, x_to_dim, x_from_dim, hidden_dim,):  \n",
        "        # To complete\n",
        "        super(Attention, self).__init__()\n",
        "        self.scale = 1.0 / math.sqrt(hidden_dim)\n",
        "        self.to_q = nn.Linear(x_to_dim, hidden_dim, bias=False)\n",
        "        self.to_k = nn.Linear(x_from_dim, hidden_dim, bias=False)\n",
        "        self.to_v = nn.Linear(x_from_dim, hidden_dim, bias=False)\n",
        "        self.to_out = nn.Linear(hidden_dim, x_to_dim)\n",
        "        \n",
        "    def forward(self, x_to, x_from):\n",
        "        # x_to = [batch size, x_to_len, x_to_dim]\n",
        "        # x_from = [batch size, x_from_len, x_from_dim]\n",
        "\n",
        "        # To complete\n",
        "        Q = self.to_q(x_to)  # (batch size, x_to_len, hidden_dim)\n",
        "        K = self.to_k(x_from)  # (batch size, x_from_len, hidden_dim)\n",
        "        V = self.to_v(x_from)  # (batch size, x_from_len, hidden_dim)\n",
        "        \n",
        "        similarity = torch.einsum('bqd,bkd->bqk', Q, K)\n",
        "        similarity = similarity * self.scale\n",
        "        attention_weights = F.softmax(similarity, dim=-1)  # [batch_size, x_to_len, x_from_len]\n",
        "        out = torch.einsum('bqk,bkd->bqd', attention_weights, V)  # [batch_size, x_to_len, hidden_dim]\n",
        "        out = self.to_out(out)  # [batch_size, x_to_len, x_to_dim]\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multi-head attention\n",
        "\n",
        "We improve the above attention implementation by introducing multi-head attention. The idea here is that we compute the attention on subspaces of the $Q,K,V$ triplets. \n",
        "We split each vector in $n$ subsets and compute the attention for each subset. At the end, we concatenate every attention output and project it with an output projection.\n",
        "\n",
        "#### Question 2. \n",
        "Implement Multihead attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, x_to_dim, x_from_dim, hidden_dim, n_heads):\n",
        "        # To complete\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.n_heads = n_heads\n",
        "        assert hidden_dim % n_heads == 0, \"hidden_dim must be divisible by n_heads\"\n",
        "        self.head_dim = hidden_dim // n_heads\n",
        "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
        "\n",
        "        self.to_q = nn.Linear(x_to_dim, hidden_dim, bias=False)\n",
        "        self.to_k = nn.Linear(x_from_dim, hidden_dim, bias=False)\n",
        "        self.to_v = nn.Linear(x_from_dim, hidden_dim, bias=False)\n",
        "        self.to_out = nn.Linear(hidden_dim, x_to_dim)\n",
        "\n",
        "    def forward(self, x_to, x_from):\n",
        "        # x_to = [batch size, x_to_len, x_to_dim]\n",
        "        # x_from = [batch size, x_from_len, x_from_dim]\n",
        "\n",
        "        # To complete\n",
        "        batch_size = x_to.size(0)\n",
        "        Q = self.to_q(x_to)  # (batch size, x_to_len, hidden_dim)\n",
        "        K = self.to_k(x_from)  # (batch size, x_from_len, hidden_dim)\n",
        "        V = self.to_v(x_from)  # (batch size, x_from_len, hidden_dim)\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)  # (batch size, n_heads, x_to_len, head_dim)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)  # (batch size, n_heads, x_from_len, head_dim)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)  # (batch size, n_heads, x_from_len, head_dim)\n",
        "        similarity = torch.einsum('bhqd,bhkd->bhqk', Q, K)\n",
        "        similarity = similarity * self.scale\n",
        "        attention_weights = F.softmax(similarity, dim=-1)  # [batch_size, n_heads, x_to_len, x_from_len]\n",
        "        out = torch.einsum('bhqk,bhkd->bhqd', attention_weights, V)  # [batch_size, n_heads, x_to_len, head_dim]\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.head_dim)  # [batch_size, x_to_len, hidden_dim]\n",
        "        out = self.to_out(out)  # [batch_size, x_to_len, x_to_dim]  \n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MultiheadAttention is the attention that is used in transformers in pratice. It is used in 2 flavors:\n",
        "- Self Attention: When $X_{to}$ attends itself ($X_{to}=X_{from}$)\n",
        "- Cross Attention. $X_{to}\\neq X_{from}$\n",
        "\n",
        "\n",
        "#### Question 3. \n",
        "Implement MultiHead Self Attention and MultiHeadCrossAttention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(MultiHeadAttention):\n",
        "    def __init__(self, x_dim, hidden_dim, n_heads):\n",
        "        # To complete\n",
        "        super(MultiHeadSelfAttention, self).__init__(x_dim, x_dim, hidden_dim, n_heads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = [batch size, x_len, x_dim]\n",
        "        # To complete\n",
        "        return super(MultiHeadSelfAttention, self).forward(x, x)\n",
        "    \n",
        "class MultiHeadCrossAttention(MultiHeadAttention):\n",
        "    def __init__(self, x_to_dim, x_from_dim, hidden_dim, n_heads):\n",
        "        # To complete\n",
        "        super(MultiHeadCrossAttention, self).__init__(x_to_dim, x_from_dim, hidden_dim, n_heads)\n",
        "\n",
        "    \n",
        "    def forward(self, x_to, x_from):\n",
        "        # x_to = [batch size, x_to_len, x_to_dim]\n",
        "        # x_from = [batch size, x_from_len, x_from_dim]\n",
        "        \n",
        "        # To complete\n",
        "        return super(MultiHeadCrossAttention, self).forward(x_to, x_from)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LayerNorm\n",
        "Another key component of the transformer is the LayerNorm. As we have previously seen, normalizing the output of a deep learning layer helps a lot with convergence and stability. \n",
        "Until Transformers, the most used normalization is BatchNorm. We normalize the data among the batch dimension. However, this has a few problems.\n",
        "- The normalization depend on the other samples in the batch\n",
        "- When using multiple GPUs, BatchNorm needs to synchronize the batch statistic across GPUs, which locks the forward process and slow down training.\n",
        "\n",
        "The last element is the most important one. Transformers, aims to be a easy to parralilize architecture and can't afford to use batchnorm.\n",
        "\n",
        "Instead, Transformers uses Layer Norm. LayerNorm is sample dependent, which removes the synchronization issue. We normalize over the channel dimension instead of the batch dimension.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*gat8a-TUnopoYN_veGEi0w.png\">\n",
        "\n",
        "To account for the loss of capacity, we map the output by a linear transformation with a learned bias and scale.\n",
        "\n",
        "#### Question 4.\n",
        "Implement the LayerNorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    # To complete\n",
        "    def __init__(self, dim, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.eps = eps\n",
        "        self.gamma = nn.Parameter(torch.ones(dim))\n",
        "        self.beta = nn.Parameter(torch.zeros(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        variance = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        x_normalized = (x - mean) / torch.sqrt(variance + self.eps)\n",
        "        return self.gamma * x_normalized + self.beta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feed Feedward Network\n",
        "\n",
        "Finally, the last block is a feed-forward network with one hidden layer. This layer has usually a size of $2 * input\\_dim$. This is followed by a dropout layer and an activation function. Here, we will use leaky relu, with a leak parameter of 0.1.\n",
        "\n",
        "#### Question 5.\n",
        "Implement the FFN layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FFN(nn.Sequential):\n",
        "    def __init__(self, hidden_dim, dropout_rate=0.1, expansion_factor=2):\n",
        "        # To complete\n",
        "        super(FFN, self).__init__(\n",
        "            nn.Linear(hidden_dim, hidden_dim * expansion_factor),\n",
        "            nn.LeakyReLU(negative_slope=0.1),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dim * expansion_factor, hidden_dim),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Personal note: we could also define the class with the following code:\n",
        "\n",
        "# class FFN(nn.Module):\n",
        "#     def __init__(self, hidden_dim, dropout_rate=0.1, expansion_factor=2):\n",
        "#         super().__init__()\n",
        "#         self.linear1 = nn.Linear(hidden_dim, hidden_dim * expansion_factor)\n",
        "#         self.activation = nn.LeakyReLU(negative_slope=0.1)\n",
        "#         self.dropout1 = nn.Dropout(dropout_rate)\n",
        "#         self.linear2 = nn.Linear(hidden_dim * expansion_factor, hidden_dim)\n",
        "#         self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.linear1(x)\n",
        "#         x = self.activation(x)\n",
        "#         x = self.dropout1(x)\n",
        "#         x = self.linear2(x)\n",
        "#         x = self.dropout2(x)\n",
        "#         return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Transformer block\n",
        "\n",
        "The last thing that we are missing are the skip connection. Like in ResNet, the transformer architecture implements the skip-connection. This allow for a better gradient flow avoiding vanishing gradient.\n",
        "There is a skip connection after the attention and the feed forward network\n",
        "\n",
        "#### Question 6.\n",
        "Given at the transformer figure at the top, implement the Transformer Encoder Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self ,data_dim, hidden_dim, n_heads, dropout_rate=0.1):\n",
        "        # To complete\n",
        "        super(TransformerEncoderBlock, self).__init__()\n",
        "        self.mhsa = MultiHeadSelfAttention(data_dim, hidden_dim, n_heads)\n",
        "        self.norm1 = LayerNorm(data_dim)\n",
        "        self.ffn = FFN(hidden_dim, dropout_rate)\n",
        "        self.norm2 = LayerNorm(data_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = [batch size, x_len, hidden dim]\n",
        "        # To complete\n",
        "        x_residual = x\n",
        "        x = self.mhsa(x)\n",
        "        x = x + x_residual\n",
        "        x = self.norm1(x)\n",
        "        x_residual = x\n",
        "        x = self.ffn(x)\n",
        "        x = x + x_residual\n",
        "        x = self.norm2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Positional embedding\n",
        "The transformers architecture is permutation independent. That means that for every token, we can swap 2 tokens and have the exact same result. However, the position of the token can be a very important information to consider. Imagine in an image. If a pixel is nearby another pixel, we want the transformer to be able to capture such information. Which is not the case for now.\n",
        "That's why we introduce positional encodings. For each token, add the positional encoding to the original token:\n",
        "\n",
        "$$\n",
        "X_i = X_i + PE(i)\n",
        "$$\n",
        "\n",
        "with X_i the token at the i dimension.\n",
        "\n",
        "The most used positional encodings are sinusoidal encodings. They are defined as follow:\n",
        "\n",
        "$$\n",
        "PE(i, 2j) = sin(i / 10000^{\\frac{2j}{d}}) \\\\\n",
        "PE(i, 2j + 1) = cos(i / 10000^{\\frac{2j}{d}})\n",
        "$$\n",
        "\n",
        "\n",
        "Where $d$ the dimension of the tokens, $i$, the i-th token in the sequence and $2j$ (resp $2j + 1$), the index of the dimension of the vector.\n",
        "The idea here is that we add a sinusoidal that encode the position in a multidimensional array.\n",
        "\n",
        "Another common positional encodings is the learned positional encoding. Simply, we let the network learn a set of tensor $PE$ that match the sequence length and dimension of the tokens.\n",
        "\n",
        "#### Question 7. \n",
        "\n",
        "Implement both Sinusoidal and Learned positional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        # To complete\n",
        "        super(SinusoidalPositionalEncoding, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = [batch size, seq len, hidden dim]\n",
        "        # To complete\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        position = torch.arange(0, seq_len, device=x.device).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, self.hidden_dim, 2, device=x.device) * -(math.log(10000.0) / self.hidden_dim))\n",
        "        pe = torch.zeros(seq_len, self.hidden_dim, device=x.device)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).expand(batch_size, -1, -1)\n",
        "        x = x + pe\n",
        "        return x\n",
        "\n",
        "class LearnedPositionalEncoding(nn.Module):\n",
        "    def __init__(self, hidden_dim, max_len):\n",
        "        # To complete\n",
        "        super(LearnedPositionalEncoding, self).__init__()\n",
        "        self.pos_embedding = nn.Embedding(max_len, hidden_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x = [batch size, seq len, hidden dim]\n",
        "        # To complete\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        positions = torch.arange(0, seq_len, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
        "        pos_embeddings = self.pos_embedding(positions)\n",
        "        x = x + pos_embeddings\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The transformer encoder\n",
        "Now you have everything you need to implement the transformer . You add positional encoding to the tokens and then stack N transformer encoder layers\n",
        "\n",
        "#### Question 8. \n",
        "Implement the transformer encoder with n_layers and the ability to choose both positional embeddings.\n",
        "\n",
        "Tip: Look into `ModuleList`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, data_dim,  hidden_dim, n_heads, n_layers, dropout_rate=0.1, positional_encoding=\"sinusoidal\", max_len=1000):\n",
        "        # To complete\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        if positional_encoding == \"sinusoidal\":\n",
        "            self.pos_encoding = SinusoidalPositionalEncoding(hidden_dim)\n",
        "        elif positional_encoding == \"learned\":\n",
        "            self.pos_encoding = LearnedPositionalEncoding(hidden_dim, max_len)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown positional_encoding type\")\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerEncoderBlock(data_dim, hidden_dim, n_heads, dropout_rate) for _ in range(n_layers)\n",
        "        ])\n",
        "        \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x = [batch size, seq len, hidden dim]\n",
        "        # To complete\n",
        "        x = self.pos_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IV: Vision Transformers (ViT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The above architecture was introduced in 2017 to process sequences of text tokens. However, it could be useful to be able to leverage this architecture for computer vision.\n",
        "\n",
        "This could be interesting to leverage to improve vision systems. If we learn the biases from the data, we can hope to have better performances. We however need compute and a lot of data to do this.\n",
        "\n",
        "To apply the transformer to images, one key question remains to be answered: How do we transform an image to tokens? The approach introduce in Vision Transformers is to cut the image into patches that are then transformed into a token trhought a linear projection.\n",
        "\n",
        "We also add an extra token, known as the classification token, that will be the token which will be use to predict upon. After going through the N transformer layers, this is the token that goes throught a multi layer perceptron.\n",
        "\n",
        "\n",
        "<img src= \"https://1.bp.blogspot.com/-_mnVfmzvJWc/X8gMzhZ7SkI/AAAAAAAAG24/8gW2AHEoqUQrBwOqjhYB37A7OOjNyKuNgCLcBGAsYHQ/s1600/image1.gif\" width=\"512\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Question 9\n",
        "\n",
        "Implement the PatchEmbedding class that allows to transform an image into a sequence of patches. \n",
        "\n",
        "Hint: Use a Conv2D with the right kernel size and stride to do the linear projection of non-overlapping patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PatchEmbeddings(nn.Module):\n",
        "    def __init__(self, img_size=96, patch_size=16, hidden_dim=512):\n",
        "        super().__init__()\n",
        "        # To complete\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(in_channels=3, out_channels=hidden_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # To complete\n",
        "        batch_size = x.size(0)\n",
        "        x = self.proj(x) \n",
        "        x = x.flatten(2) \n",
        "        x = x.transpose(1, 2)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Question 10\n",
        "\n",
        "Implement the vision transformer\n",
        "\n",
        "Hint: Use Conv2D with the right kernel size and stride to do the linear projection of non-overlapping patches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self, img_size, patch_size, hidden_dim, n_heads, n_layers, n_classes, dropout_rate):\n",
        "        super(ViT, self).__init__()\n",
        "\n",
        "        self.patch_embeddings = PatchEmbeddings(img_size, patch_size, hidden_dim)\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, hidden_dim))\n",
        "\n",
        "        self.transformer_encoder = TransformerEncoder(\n",
        "            data_dim=hidden_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            n_heads=n_heads,\n",
        "            n_layers=n_layers,\n",
        "            dropout_rate=dropout_rate,\n",
        "            positional_encoding=\"learned\",\n",
        "            max_len=(img_size // patch_size) ** 2 + 1\n",
        "        )\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Linear(hidden_dim, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, apply_mlp_head=True):\n",
        "        x = self.patch_embeddings(x)\n",
        "\n",
        "        B = x.size(0)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        x = self.transformer_encoder(x)\n",
        "\n",
        "        if apply_mlp_head:\n",
        "            x = self.mlp_head(x[:, 0])\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Question 11 \n",
        "Train a ViT on CIFAR10 for 100 epochs (for compute reason you can use only 20 epochs) and log both train and test loss and accuracy. \n",
        "We provide a data augmentation strategy called auto augment to avoid overfitting on the training data.\n",
        "Hparameters are to be choosen to your discretion.\n",
        "\n",
        "\n",
        "Tips for Hparams:\n",
        "- Don't use a transformer hidden dim too big (<256)\n",
        "- Use a small patch size\n",
        "- Use AdamW with some weight decay to avoid overfitting\n",
        "- Use between 2 and 6 transformer layers.\n",
        "- Use between 2 and 4 transformer heads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "train_set = CIFAR10(root='./data', train=True, download=True, transform=transforms.Compose([\n",
        "    transforms.autoaugment.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
        "]))\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "test_set = CIFAR10(root='./data', train=False, download=True, transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
        "]))\n",
        "\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20:   0%|          | 1/391 [00:00<00:46,  8.37it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20: 100%|██████████| 391/391 [00:09<00:00, 43.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Loss: 1.9162\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy after epoch 1: 45.84%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20: 100%|██████████| 391/391 [00:09<00:00, 43.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/20], Loss: 1.6543\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy after epoch 2: 49.65%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20: 100%|██████████| 391/391 [00:09<00:00, 42.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/20], Loss: 1.5428\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy after epoch 3: 53.20%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/20: 100%|██████████| 391/391 [00:09<00:00, 42.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/20], Loss: 1.4688\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy after epoch 4: 56.22%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/20: 100%|██████████| 391/391 [00:09<00:00, 42.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/20], Loss: 1.4049\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy after epoch 5: 56.87%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/20: 100%|██████████| 391/391 [00:09<00:00, 41.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/20], Loss: 1.3490\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy after epoch 6: 57.57%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/20: 100%|██████████| 391/391 [00:09<00:00, 41.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/20], Loss: 1.3091\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy after epoch 7: 58.66%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/20: 100%|██████████| 391/391 [00:09<00:00, 41.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [8/20], Loss: 1.2671\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy after epoch 8: 61.70%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/20: 100%|██████████| 391/391 [00:09<00:00, 41.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [9/20], Loss: 1.2278\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy after epoch 9: 61.23%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/20: 100%|██████████| 391/391 [00:09<00:00, 41.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/20], Loss: 1.1865\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy after epoch 10: 63.27%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/20: 100%|██████████| 391/391 [00:09<00:00, 40.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [11/20], Loss: 1.1603\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy after epoch 11: 63.20%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/20: 100%|██████████| 391/391 [00:09<00:00, 40.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [12/20], Loss: 1.1414\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy after epoch 12: 63.21%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/20: 100%|██████████| 391/391 [00:09<00:00, 41.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [13/20], Loss: 1.1108\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy after epoch 13: 65.01%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/20: 100%|██████████| 391/391 [00:09<00:00, 41.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [14/20], Loss: 1.0843\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy after epoch 14: 64.32%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/20: 100%|██████████| 391/391 [00:09<00:00, 41.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [15/20], Loss: 1.0591\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy after epoch 15: 64.91%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/20: 100%|██████████| 391/391 [00:09<00:00, 41.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [16/20], Loss: 1.0366\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy after epoch 16: 65.59%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/20: 100%|██████████| 391/391 [00:09<00:00, 41.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [17/20], Loss: 1.0199\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy after epoch 17: 66.26%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/20: 100%|██████████| 391/391 [00:09<00:00, 41.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [18/20], Loss: 1.0044\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy after epoch 18: 66.11%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19/20: 100%|██████████| 391/391 [00:09<00:00, 40.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [19/20], Loss: 0.9794\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy after epoch 19: 66.33%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20/20: 100%|██████████| 391/391 [00:09<00:00, 41.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [20/20], Loss: 0.9707\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy after epoch 20: 67.48%\n"
          ]
        }
      ],
      "source": [
        "# To complete: train the model, (don't forget to test it)\n",
        "\n",
        "model = ViT(\n",
        "    img_size=32,\n",
        "    patch_size=4,\n",
        "    hidden_dim=256,\n",
        "    n_heads=4,\n",
        "    n_layers=4,\n",
        "    n_classes=10,\n",
        "    dropout_rate=0.1\n",
        ").to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_epochs = 20\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(avg_loss)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_test_loss = test_loss / len(test_loader)\n",
        "    test_losses.append(avg_test_loss)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy after epoch {epoch+1}: {accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot the training and test loss\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Training and Test Loss\")\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(test_losses, label=\"Test Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# V: Vision Language Models (VLM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, we will implement a Vision Language Model and finetune one for a Visual Question Answering task.\n",
        "\n",
        "Here is a diagram for the VLM architecture:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/AviSoori1x/seemore/refs/heads/main/images/vlm.png\" width=512>\n",
        "\n",
        "We have already implemented the vision encoder. We will now implement the text decoder (also based on the transformer architecture), and a multimodal projector. We will try to create a deep understanding of what is happening."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## V-1. The Multimodal Projector \n",
        "\n",
        "The ViT class (implemented in the last section) takes an input image and returns the embedding corresponding to the class token (CLS), which is then used to condition the text generation in the language decoder. \n",
        "\n",
        "However, we can not directly concatenate this to the text embeddings. We need to project this from the dimensionality of image embeddings from the vision transformer to the dimensionality of text embeddings. This is done by the multimodal projector.\n",
        "\n",
        "This projector is usually a single learnable layer followed by a non-linearity or an MLP. Here we will implement a MLP wit one hidden layer, an expansion factor of $4$ and a GELU activation function. \n",
        "\n",
        "#### Question 12\n",
        "Implement the Multimodal Projector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiModalProjector(nn.Module):\n",
        "    # To complete\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(MultiModalProjector, self).__init__()\n",
        "        hidden_dim = input_dim * 4\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## V-2. The Language Transformer Decoder\n",
        "\n",
        "The final component we need to look at is the decoder language model. \n",
        "The (text) Transformer Decoder is similar to the (Vision) Transformer Encoder defined before. The main differences are: \n",
        "- A text token embedding replaces the patch embedding. \n",
        "- Causal Self Attention replaces Self Attention in the Transformer Block. \n",
        "- A language modeling head is added on top of the last Transformer Block. \n",
        "\n",
        "\n",
        "### Causal Attention\n",
        "\n",
        "In Causal Attention, masking is applied in each attention head to obscure any information following the current token's position, thereby directing the model's attention to only the preceding parts of the sequence. A token can not attend to the \"future\" (following) tokens in the sentence. \n",
        "\n",
        "In practice, a lower triangular mask $M$ is added to the similarity matrix between $Q$ and $K$. \n",
        "\n",
        "The causal attention operation is then given by: \n",
        "$$\n",
        "A_{causal}(Q,K,V) = \\text{SoftMax}(\\frac{QK^T + M}{\\sqrt{d_k}})V\n",
        "$$\n",
        "\n",
        "<img src=\"https://blog.sailor.plus/deep-learning/images/1613723693323.png\" width=512> \n",
        "\n",
        "\n",
        "#### Question 13\n",
        "Implement the Causal Multi Head Self Attention \n",
        "\n",
        "Hint: You can start from the class implemented before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CausalMultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, x_dim, hidden_dim, n_heads):\n",
        "        super(CausalMultiHeadSelfAttention, self).__init__()\n",
        "        self.n_heads = n_heads\n",
        "        assert hidden_dim % n_heads == 0\n",
        "        self.head_dim = hidden_dim // n_heads\n",
        "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
        "\n",
        "        self.to_q = nn.Linear(x_dim, hidden_dim, bias=False)\n",
        "        self.to_k = nn.Linear(x_dim, hidden_dim, bias=False)\n",
        "        self.to_v = nn.Linear(x_dim, hidden_dim, bias=False)\n",
        "        self.to_out = nn.Linear(hidden_dim, x_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, x_len, x_dim)\n",
        "        batch_size, x_len, _ = x.size()\n",
        "\n",
        "        Q = self.to_q(x)\n",
        "        K = self.to_k(x)\n",
        "        V = self.to_v(x)\n",
        "\n",
        "        Q = Q.view(batch_size, x_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, x_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, x_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        similarity = torch.einsum('bhqd,bhkd->bhqk', Q, K)\n",
        "        similarity = similarity * self.scale\n",
        "\n",
        "        # --- causal mask ---\n",
        "        mask = torch.tril(torch.ones(x_len, x_len, device=x.device))\n",
        "        similarity = similarity.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn = torch.softmax(similarity, dim=-1)\n",
        "        out = torch.einsum('bhqk,bhkd->bhqd', attn, V)\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        out = out.view(batch_size, x_len, -1)\n",
        "\n",
        "        return self.to_out(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformer Decoder Block\n",
        "\n",
        "Now we implement a Transformer Decoder Block using the Causal Attention. \n",
        "\n",
        "#### Question 14\n",
        "\n",
        "Implement the Transformer Decoder Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerDecoderBlock(nn.Module):\n",
        "    def __init__(self, data_dim, hidden_dim, n_heads, dropout_rate=0.1):\n",
        "        # To complete\n",
        "        super(TransformerDecoderBlock, self).__init__()\n",
        "        self.causal_mhsa = CausalMultiHeadSelfAttention(data_dim, hidden_dim, n_heads)\n",
        "        self.norm1 = LayerNorm(data_dim)\n",
        "        self.cross_mha = MultiHeadCrossAttention(data_dim, data_dim, hidden_dim, n_heads)\n",
        "        self.norm2 = LayerNorm(data_dim)\n",
        "        self.ffn = FFN(hidden_dim, dropout_rate)\n",
        "        self.norm3 = LayerNorm(data_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # To complete\n",
        "        x_residual = x\n",
        "        x = self.causal_mhsa(x)\n",
        "        x = x + x_residual\n",
        "        x = self.norm1(x)\n",
        "        x_residual = x\n",
        "        x = self.cross_mha(x, x)\n",
        "        x = x + x_residual\n",
        "        x = self.norm2(x)\n",
        "        x_residual = x\n",
        "        x = self.ffn(x)\n",
        "        x = x + x_residual\n",
        "        x = self.norm3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building the Language Transformer Decoder\n",
        "\n",
        "#### Question 15\n",
        "\n",
        "Implement the Language Transformer Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LanguageTransformerDecoder(nn.Module):\n",
        "    def __init__(self, n_embd, image_embed_dim, vocab_size, n_heads, n_layers):\n",
        "        # To complete\n",
        "        super(LanguageTransformerDecoder, self).__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
        "        self.pos_encoding = LearnedPositionalEncoding(n_embd, max_len=1000)\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerDecoderBlock(n_embd, n_embd, n_heads) for _ in range(n_layers)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx, image_embeds):\n",
        "        # To complete\n",
        "        x = self.token_embedding(idx)\n",
        "        x = self.pos_encoding(x)\n",
        "    \n",
        "    def generate(self, idx, image_embeds, max_new_tokens):\n",
        "        ### Do not complete this method ### \n",
        "        # With the logits outputted by the forward method \n",
        "        # and using sampling methods (e.g., greedy sampling, top-k sampling, nucleus sampling)\n",
        "        # we can generate some text tokens!! \n",
        "        for _ in range(max_new_tokens):\n",
        "            logits = self.forward(idx, image_embeds)\n",
        "            logits = logits[:, -1, :]  # Focus on the last time step\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.argmax(probs, dim=-1, keepdim=True)\n",
        "            idx = torch.cat((idx, next_token), dim=1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the logits outputted by the forward method and using sampling methods (e.g., greedy sampling, top-k sampling, nucleus sampling), we can generate some text tokens!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## V-4. Bringing everything together to implement the Vision Language Model\n",
        "\n",
        "Now, we have all the element to build our Vision Languauge model. \n",
        "\n",
        "#### Question 16\n",
        "\n",
        "Implement the Vision Language Model class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VisionLanguageModel(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_embd,\n",
        "            image_embed_dim,\n",
        "            vocab_size,\n",
        "            n_enc_layers,\n",
        "            img_size, patch_size,\n",
        "            n_heads,\n",
        "            n_dec_layers,\n",
        "            dropout_rate\n",
        "        ):\n",
        "        # To complete\n",
        "        pass\n",
        "\n",
        "    def forward(self, img_array, idx):\n",
        "        # To complete\n",
        "        pass\n",
        "\n",
        "    def generate(self, img_array, idx, max_new_tokens):\n",
        "        # To complete\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## VI. Finetuning a VLM for Visual Question Answering task\n",
        "\n",
        "Training a Vision Language Model from scratch requires a lot of training data and GPU ressources. Then, we are going to finetune an already pretrained VLM on a specific task: Visual Question Answering.  \n",
        "\n",
        "We will fine-tune the recent VLM `Florence-2` from Microsoft on the dataset DocVQA.\n",
        "\n",
        "Lets start by donwloading the model and the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoProcessor\n",
        "import torch\n",
        "\n",
        "data = load_dataset(\"HuggingFaceM4/DocumentVQA\", split=[\"train[:10%]\", \"validation[:10%]\", \"test[:10%]\"])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-base-ft\", trust_remote_code=True, revision='refs/pr/6').to(device)\n",
        "processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base-ft\", trust_remote_code=True, revision='refs/pr/6')\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's do inference with our dataset first to see how the model performs before fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to run the model on an example\n",
        "def run_example(task_prompt, text_input, image):\n",
        "    prompt = task_prompt + text_input\n",
        "\n",
        "    # Ensure the image is in RGB mode\n",
        "    if image.mode != \"RGB\":\n",
        "        image = image.convert(\"RGB\")\n",
        "\n",
        "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
        "    generated_ids = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        pixel_values=inputs[\"pixel_values\"],\n",
        "        max_new_tokens=1024,\n",
        "        num_beams=3\n",
        "    )\n",
        "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "    parsed_answer = processor.post_process_generation(generated_text, task=task_prompt, image_size=(image.width, image.height))\n",
        "    return parsed_answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for idx in range(3):\n",
        "    print(run_example(\"DocVQA\", 'What is written on top of the document?', data[0][idx]['image']))\n",
        "    display(data[0][idx]['image'].resize([350, 350]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to construct our dataset. Note how we are adding a new task prefix `<DocVQA>` before the question when constructing the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DocVQADataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.data[idx]\n",
        "        question = \"<DocVQA>\" + example['question']\n",
        "        first_answer = example['answers'][0]\n",
        "        image = example['image']\n",
        "        if image.mode != \"RGB\":\n",
        "            image = image.convert(\"RGB\")\n",
        "        return question, first_answer, image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's get to fine-tuning. We will instntiate our dataset, the data collator, and start training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoProcessor, get_scheduler\n",
        "from bitsandbytes.optim import AdamW\n",
        "\n",
        "def collate_fn(batch):\n",
        "    questions, answers, images = zip(*batch)\n",
        "    inputs = processor(text=list(questions), images=list(images), return_tensors=\"pt\", padding=True).to(device)\n",
        "    return inputs, answers\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = DocVQADataset(data[0])\n",
        "val_dataset = DocVQADataset(data[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finetuning the model on the entire dataset will be too long for us (around 2,5 hours per epoch) and could be too heavy for the vRAM of the GPU we are using. We reduce the size of the dataset using `Subset`.\n",
        "\n",
        "You can adapt the size of the subsets depending on your GPU. \n",
        "\n",
        "If you have time, run the finetuning on the entire dataset, the results will be even better!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use a subset of the dataset for training\n",
        "train_dataset = Subset(train_dataset, list(range(0, 1000)))\n",
        "val_dataset = Subset(val_dataset, list(range(0, 200)))\n",
        "\n",
        "# Create DataLoader\n",
        "batch_size = 2\n",
        "num_workers = 0\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, num_workers=num_workers, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn, num_workers=num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Question 16 (Bonus)\n",
        "\n",
        "Complete the training loop in the `train model` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(train_loader, val_loader, model, processor, epochs=10, lr=1e-6):\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    num_training_steps = epochs * len(train_loader)\n",
        "    lr_scheduler = get_scheduler(\n",
        "        name=\"linear\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_training_steps,\n",
        "    )\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        i = -1\n",
        "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n",
        "            i += 1\n",
        "            inputs, answers = batch\n",
        "\n",
        "            input_ids = inputs[\"input_ids\"]\n",
        "            pixel_values = inputs[\"pixel_values\"]\n",
        "            labels = processor.tokenizer(text=answers, return_tensors=\"pt\", padding=True, return_token_type_ids=False).input_ids.to(device)\n",
        "\n",
        "            ### Complete here:  ###\n",
        "            # Get the logits from the model\n",
        "            # Calculate the loss\n",
        "            # Backpropagate\n",
        "\n",
        "\n",
        "            ### End completion ###\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        print(f\"Average Training Loss: {avg_train_loss}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{epochs}\"):\n",
        "                inputs, answers = batch\n",
        "\n",
        "                input_ids = inputs[\"input_ids\"]\n",
        "                pixel_values = inputs[\"pixel_values\"]\n",
        "                labels = processor.tokenizer(text=answers, return_tensors=\"pt\", padding=True, return_token_type_ids=False).input_ids.to(device)\n",
        "\n",
        "                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
        "                loss = outputs.loss\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        print(f\"Average Validation Loss: {avg_val_loss}\")\n",
        "\n",
        "        # Save model checkpoint\n",
        "        output_dir = f\"./model_checkpoints/epoch_{epoch+1}\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        model.save_pretrained(output_dir)\n",
        "        processor.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will freeze image encoder for this TP. The authors have reported improvement in unfreezing image encoder, but note that this will result in more resource usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for param in model.vision_tower.parameters():\n",
        "    param.is_trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: if the following cell crash with the error `OutOfMemoryError: CUDA out of memory.`, try to reduce the batch size and/or the number of example in the train/validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_model(train_loader, val_loader, model, processor, epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's do inference with our finetuned model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for idx in range(3):\n",
        "    print(run_example(\"DocVQA\", 'What is written on top of the document?', data['train'][idx]['image']))\n",
        "    display(data['train'][idx]['image'].resize([350, 350]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM/ea6h3qQ+sqFVLFEhtusQ",
      "collapsed_sections": [],
      "name": "INF473V-TD6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.25"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
