{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrz68bnPPtVK"
      },
      "source": [
        "# Lab 4: Prompt-learning and LoRA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaZjtufgPtVL"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHIUV_yXPtVL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import clip\n",
        "import pyarrow as pa\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from timm.data.constants import (\n",
        "    IMAGENET_DEFAULT_MEAN,\n",
        "    IMAGENET_DEFAULT_STD,\n",
        ")\n",
        "\n",
        "CLASSES_CIFAR10 = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1rIjvGVPtVN"
      },
      "source": [
        "# Part 1. Prompt/Few-Shot learning\n",
        "\n",
        "In this part, we will focus on prompt/few-shot learning with CLIP on the CIFAR-10 dataset.\n",
        "\n",
        "### **Question 1.1**\n",
        "\n",
        "Evaluate CLIP zero-shot performances on CIFAR-10.\n",
        "\n",
        "Tip.  CIFAR-10 classes are stored in the list `CLASSES_CIFAR10`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6sqF6maPtVN"
      },
      "outputs": [],
      "source": [
        "# - function to evaluate CLIP\n",
        "def evaluate_clip(model, test_loader, classes, prefix, device):\n",
        "    \"\"\"\n",
        "    Function to evaluate the model in a zero-shot setting.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : clip model\n",
        "        The model to evaluate.\n",
        "    test_loader : torch.utils.data.DataLoader\n",
        "        The test loader.\n",
        "    classes : list\n",
        "        List of classes.\n",
        "    prefix : str\n",
        "        Prefix to add to the classes.\n",
        "        e.g. \"This a photo of\"\n",
        "    \"\"\"\n",
        "    ### Comeplete the following code\n",
        "    print(f\"Test Acc: {test_acc:.3f}\")\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIHqgxVxPtVN"
      },
      "outputs": [],
      "source": [
        "# - load the model\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# - load the data\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "     transforms.Resize(size=(224, 224)),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5,), (0.5,)),\n",
        "])\n",
        "\n",
        "test_set = torchvision.datasets.CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECW6SZR7PtVN"
      },
      "outputs": [],
      "source": [
        "evaluate_clip(model,\n",
        "              test_loader=test_loader,\n",
        "              classes=[\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"],\n",
        "              prefix=\"\",\n",
        "              device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIB925BxPtVN"
      },
      "source": [
        "### **Question 1.2**\n",
        "Explore 2 or 3 prompts to evaluate CLIP on CIFAR-10, e.g. you can use \"A photo of {CLASS}\".\n",
        "How much can you gain by changing the prompts?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t53iH3ugPtVN"
      },
      "outputs": [],
      "source": [
        "### Comeplete the following code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFRQRKyFPtVN"
      },
      "outputs": [],
      "source": [
        "### Comeplete the following code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fth8FPuEPtVN"
      },
      "source": [
        "## CoOp\n",
        "\n",
        "As you may have noticed, changing the prompt can significantly impact the model's performance. However, crafting the perfect prompt is a challenging task that requires extensive experimentation. To address this issue, Context Optimization (CoOp) [Zhou et al, 2022] introduced a novel method for adapting CLIP to downstream classification tasks, bypassing the need for manual prompt engineering.\n",
        "\n",
        "CoOp aims to learn a prompt called **learnable context** (only a few trainable parameters compared to finetuning the entire model)  to enhance the model's performance on the downstream task.\n",
        "\n",
        "\n",
        "Here is a figure of the CoOp approach:\n",
        "\n",
        "<img src=\"https://jumdc.github.io/assets/img/coop.png\" width=768>\n",
        "\n",
        "\n",
        "The prompt given to the text encoder $g(Â·)$ will be of the form $t = [\\text{SOS}][V]_1[V]_2 . . . [V]_M[\\text{CLASS}][\\text{EOS}]$ where each $[V]_m, (m âˆˆ {1, . . . , M})$ is a vector with the same dimension as word embeddings, $M$ is a hyperparameter specifying the number of context tokens, [SOS] and [EOS] are the special tokens indicating the start and end of the sentence (the same as in the pre-trained CLIP model). The context tokens $[V]_m$ are initialized randomly and learned during training. By forwarding a prompt $t$ to the text encoder g(Â·), we can obtain a classification weight vector representing a visual concept (still from the [EOS] token position). The prompt can then be trained using a cross-entropy between the classification weight vector and the one-hot label vector of the input image.\n",
        "\n",
        "\n",
        "Let's now implement the CoOp approach.\n",
        "Code-wise CoOp relies on a pretrained CLIP with a few modifications.\n",
        "\n",
        "Note on [SOS] and [EOS] tokens:\n",
        "- Tokenization is the process of converting text into a sequence of tokens, which can be words, subwords, or characters. These tokens are the smallest units of meaning in a text that can be processed by a language model. For example, the sentence â€œHello, world!â€ can be tokenized into [â€œHelloâ€, â€œ,â€, â€œworldâ€, â€œ!â€]. Tokenization simplifies the text and allows the model to work with manageable chunks of data.\n",
        "- The [SOS] token is a special token that indicates the start of a sentence. The [EOS] token is a special token that indicates the end of a sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSvQDTTZPtVN"
      },
      "source": [
        "### **Question 1.3**\n",
        "\n",
        "Implement the `LearnableContext` class. This class is designed to create the learnable prompt for a CLIP model, enabling it to adapt dynamically to specific classification tasks.\n",
        "\n",
        "The LearnableContext class should do the following:\n",
        "\n",
        "- Initializes a set of learnable context tokens (ctx) to adapt prompts dynamically.\n",
        "- Prepares token embeddings that include a prefix ([SOS] token embedding) and a suffix (class name and [EOS] tokens embedding) for each class.\n",
        "- Constructs the full prompt embeddings by combining the prefix, learnable context, and suffix during the forward pass.\n",
        "\n",
        "Example of the learnable context for CIFAR-10:\n",
        "> let `X` be a learnable context token, with `n_ctx=3`, the prompt for the class \"airplane\" would be: `[SOS] X X X airplane [EOS]` and for the class \"automobile\": `[SOS] X X X automobile [EOS]`.\n",
        "\n",
        "Tip. You can use `clip_model.token_embedding()` to get the token embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3aS41uWPtVO"
      },
      "outputs": [],
      "source": [
        "class LearnableContext(nn.Module):\n",
        "    def __init__(self,\n",
        "                 classnames,\n",
        "                 clip_model,\n",
        "                 device,\n",
        "                 n_ctx=16):\n",
        "        \"\"\"\n",
        "        classnames: list of str\n",
        "            A list of class names for classification.\n",
        "        clip_model: torch.nn.Module\n",
        "            Pretrained CLIP model.\n",
        "        device: str\n",
        "            Device to use.\n",
        "        n_ctx: int\n",
        "            Number of context words to learn.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # To complete\n",
        "        pass\n",
        "\n",
        "    def forward(self):\n",
        "        \"\"\"Forward pass of the LearnableContext.\n",
        "\n",
        "        Returns:\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            The context vector for each class: [SOS] ctx [CLASS] [EOS]\n",
        "        \"\"\"\n",
        "        # To complete\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzRO7RoAPtVO"
      },
      "source": [
        "### **Question 1.4**\n",
        "\n",
        "Implement the CoOp model.\n",
        "The CoOpModel class performs the following tasks:\n",
        "- Prompt Learning: Uses LearnableContext to generate learnable context tokens for each class, transforming them into dynamic text prompts.\n",
        "- Text and Image Encoding: Encodes images with CLIP's visual encoder and encodes text prompts with the model's transformer and text projection.\n",
        "- Logit Calculation: Computes cosine similarities between image and text features and outputs logits using a learned logit scale.\n",
        "\n",
        "Tip:\n",
        "You can implement the following methods in the CoOp Model class:\n",
        "- `__init__`: Initializes the CoOp model with CLIP pretrained model (`clip.load(\"ViT-B/32\", device=\"cuda\")`) and the `LearnableContext` class.\n",
        "- `encode_text`: Encodes the learnable prompts using the CLIP pretrained text encoder and projection head.\n",
        "    It should:\n",
        "    - Add the positional embedding to the input prompts (prompts).\n",
        "    - Use the transformer to process the input sequence.\n",
        "    - Apply the final layer normalization (ln_final) to the transformed sequence.\n",
        "    - Extract the features corresponding to the end-of-text [EOS] token.\n",
        "    - Project the resulting feature to the text projection space using text_projection.\n",
        "    - CLIP uses a sequence-first approach for the Text model, so the input shape to the text encoder should be (sequence_length, batch_size, hidden_size).\n",
        "- `forward`: Performs the forward pass computations for the CoOp model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtklwMt_PtVO"
      },
      "outputs": [],
      "source": [
        "class CoOpModel(nn.Module):\n",
        "    \"\"\"CoOp model\"\"\"\n",
        "    def __init__(self,\n",
        "                 classnames,\n",
        "                 clip_model,\n",
        "                 device,\n",
        "                 n_ctx=16):\n",
        "        \"\"\"Initializes the CoOp model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        classnames: list of str\n",
        "            A list of class names for classification.\n",
        "        clip_model: torch.nn.Module\n",
        "            Pretrained CLIP model.\n",
        "        device: str\n",
        "            Device to use.\n",
        "        n_ctx: int\n",
        "            Number of context words to learn.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.clip_model = clip_model\n",
        "        self.prompt_learner = LearnableContext(n_ctx=n_ctx,\n",
        "                                            device=device,\n",
        "                                            classnames=classnames,\n",
        "                                            clip_model=clip_model)\n",
        "        # To complete\n",
        "        pass\n",
        "\n",
        "    def encode_text(self, prompts, tokenized_prompts):\n",
        "        \"\"\"\n",
        "        Encodes the learnable text prompts using the CLIP model's transformer and projection layers.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        prompts: torch.Tensor\n",
        "            The learnable text prompts.\n",
        "        tokenized_prompts: torch.Tensor\n",
        "            The tokenized prompts -> the highest token in each sequence represents the [EOT].\n",
        "        \"\"\"\n",
        "        # To complete\n",
        "        pass\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"Given an image, returns the logits for each class.\"\"\"\n",
        "        # To complete\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxLHYM0rPtVO"
      },
      "source": [
        "#### **Question 1.5**\n",
        "Train CoOpModel on CIFAR-10 using a Cross Entropy loss and log both train and test loss and accuracy. Since we are in a few-shot setting, **you can only use between 1 and 20 samples per class**. You should obtain at least 85+% test accuracy.\n",
        "\n",
        "Hparameters are to be choosen to your discretion.\n",
        "\n",
        "Tips:\n",
        "- You can use `torch.utils.data.Subset` to create a subset of the CIFAR-10 dataset with a limited number of samples per class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqLsXzxuPtVO"
      },
      "outputs": [],
      "source": [
        "def coop_training_loop(n_epochs,\n",
        "                       n_shot,\n",
        "                       device,\n",
        "                       lr,\n",
        "                       n_ctx,\n",
        "                       batch_size,\n",
        "                       classes=None):\n",
        "    # -- model\n",
        "    clip_model = clip.load(\"ViT-B/32\", device=device)[0]\n",
        "    coop_clip = CoOpModel(classnames=classes,\n",
        "                        clip_model=clip_model,\n",
        "                        device=device,\n",
        "                        n_ctx=n_ctx\n",
        "                        ).to(device)\n",
        "    # -- data\n",
        "    train_transform = None # To complete\n",
        "    test_transform = None # To complete\n",
        "    train_set = torchvision.datasets.CIFAR10(root=\"./data\", train=True, transform=train_transform, download=True)\n",
        "    test_set = torchvision.datasets.CIFAR10(root=\"./data\", train=False, transform=test_transform, download=True)\n",
        "\n",
        "    # To complete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_ebIzN9PtVO"
      },
      "outputs": [],
      "source": [
        "coop_clip_trained = coop_training_loop(n_epochs=35,\n",
        "                                device=device,\n",
        "                                n_shot=20,\n",
        "                                lr=0.001,\n",
        "                                n_ctx=12,\n",
        "                                classes=CLASSES_CIFAR10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I47d-PJjZEEL"
      },
      "source": [
        "\n",
        "# Part 2: LoRA (low-rank adaptation)\n",
        "\n",
        "### **What is LoRA?**\n",
        "\n",
        "LoRA, or **Low-Rank Adaptation**, is a technique designed to **fine-tune large pre-trained models efficiently** by introducing learnable low-rank matrices into their architecture. Instead of updating all the weights of a large model (which can be computationally expensive and require a lot of storage), LoRA updates a small set of parameters while keeping the original model frozen. This drastically reduces the computational overhead and memory usage during fine-tuning.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why LoRA?**\n",
        "\n",
        "- **Efficiency:** Reduces the number of trainable parameters significantly.\n",
        "- **Scalability:** Works well for large models like transformers but can also be applied to simpler models like MLPs.\n",
        "- **Parameter Isolation:** Fine-tuning multiple tasks without interfering with each other is easier as the low-rank adapters can be task-specific.\n",
        "\n",
        "---\n",
        "\n",
        "### **How LoRA Works**\n",
        "\n",
        "1. **Key Idea:** Approximates the weight update \\( \\Delta W \\) in a neural network using the product of two low-rank matrices \\( A \\) and \\( B \\):\n",
        "   \\[\n",
        "   W' = W + \\Delta W, \\quad \\Delta W = A \\cdot B\n",
        "   \\]\n",
        "   - \\( A \\) and \\( B \\) are much smaller matrices (rank \\( r \\)) compared to the original weight matrix \\( W \\).\n",
        "\n",
        "2. **Freezing the Model:** The original modelâ€™s weights \\( W \\) remain frozen during fine-tuning. Only \\( A \\) and \\( B \\) are updated, which are much smaller in size.\n",
        "\n",
        "3. **Low-rank Design:** By choosing a small rank \\( r \\), LoRA ensures that the added parameters are minimal while still allowing the model to adapt to new tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### **LoRA in MLPs**\n",
        "\n",
        "- **Today:** Let's first do a small toy-example of LoRA on MLP to understand how LoRA works\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4IAzNqGaC4s"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "torch.manual_seed(0) # reproducibility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OG2lxbUPaQBv"
      },
      "source": [
        "Letâ€™s generate a **toy dataset** with random data for a classification task. The dataset includes a small amount of meaningful signal, allowing the modelâ€™s loss to improve during training as it learns to identify patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b355567e"
      },
      "outputs": [],
      "source": [
        "X = torch.rand((1000, 20)) # Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1).\n",
        "y = (torch.sin(X.sum(1)) > 0).long() # y is the label with shape (1000, 1) which results in 1 if the sin of sum of elements in each row is > 0.5 and 0 otherwise. y is then cast to (torch.int64)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v54HcFP23ylo"
      },
      "outputs": [],
      "source": [
        "# Distribution of data between both classes\n",
        "unique, counts = torch.unique(y, return_counts=True)\n",
        "distribution = dict(zip(unique.tolist(), counts.tolist()))\n",
        "distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a60a869d"
      },
      "outputs": [],
      "source": [
        "n_train = 800\n",
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8859572e"
      },
      "outputs": [],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    torch.utils.data.TensorDataset(X[:n_train], y[:n_train]),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "eval_dataloader = torch.utils.data.DataLoader(\n",
        "    torch.utils.data.TensorDataset(X[n_train:], y[n_train:]),\n",
        "    batch_size=batch_size,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97bddd2c"
      },
      "source": [
        "We use a simple multilayer perceptron (**MLP**). For demonstration purposes, we use a very large number of hidden units. This is totally an **overkill** for this task but it helps to demonstrate the advantages of `peft`. In more realistic settings, models will also be quite large on average, so this is not far-fetched."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b43cd8f"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            # Complete the following code for a 3 layer MLP,\n",
        "            # with a big middle layer dim > 2000\n",
        "            nn.LogSoftmax(dim=-1), #log loss / binary cross entropy loss\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.seq(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1277bf00"
      },
      "source": [
        "Here are just a few training hyper-parameters and a simple function that performs the training and evaluation loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d14c0c4"
      },
      "outputs": [],
      "source": [
        "lr = 0.002\n",
        "batch_size = 64\n",
        "max_epochs = 35\n",
        "device = 'cpu' if not torch.cuda.is_available() else 'cuda'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "657d6b3e"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, criterion, train_dataloader, eval_dataloader, epochs, device):\n",
        "    train_losses = []\n",
        "    eval_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        train_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n",
        "        for xb, yb in train_bar:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            outputs = model(xb)\n",
        "            loss = criterion(outputs, yb)\n",
        "            train_loss += loss.detach().item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        train_loss_total = train_loss / len(train_dataloader)\n",
        "        train_losses.append(train_loss_total)\n",
        "\n",
        "        # Evaluation step\n",
        "        model.eval()\n",
        "        eval_loss = 0\n",
        "\n",
        "        eval_bar = tqdm(eval_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Eval]\", leave=False)\n",
        "        for xb, yb in eval_bar:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(xb)\n",
        "                loss = criterion(outputs, yb)\n",
        "                eval_loss += loss.detach().item()\n",
        "\n",
        "        eval_loss_total = eval_loss / len(eval_dataloader)\n",
        "        eval_losses.append(eval_loss_total)\n",
        "\n",
        "    # Save the training curve\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(range(1, epochs + 1), train_losses, label=\"Train Loss\", marker=\"o\")\n",
        "    plt.plot(range(1, epochs + 1), eval_losses, label=\"Eval Loss\", marker=\"o\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training and Evaluation Loss Curve\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Training curve saved as 'training_curve.png'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvP8PSPXnahj"
      },
      "source": [
        "### **Question 2.1**:\n",
        "Please complete the following code for this toy example:\n",
        "- base_model\n",
        "- optimizer\n",
        "- criterion (loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f059ced4"
      },
      "outputs": [],
      "source": [
        "base_model = ### Complete me\n",
        "optimizer = ### Complete me\n",
        "criterion = ### Complete me"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJD-3tGW93IR"
      },
      "outputs": [],
      "source": [
        "base_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIyHe3KZ745s"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    ### Complete me\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf2aHDXtA6p3"
      },
      "source": [
        "Show trainable params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8538QGChubaZ"
      },
      "outputs": [],
      "source": [
        "print_trainable_parameters(base_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd2f_7X7nv_2"
      },
      "source": [
        "### **Question 2.2**:\n",
        "\n",
        "**Question:**\n",
        "Explain your observations regarding the training behavior and performance on the new 'MLP' regarding the training and eval loss.\n",
        "\n",
        "---\n",
        "\n",
        "**Answer your observations here:**\n",
        "\n",
        "ðŸ‘‰ **[YOUR ANSWER]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17698863"
      },
      "outputs": [],
      "source": [
        "# Lets train the base model\n",
        "train(base_model, optimizer, criterion, train_dataloader, eval_dataloader, epochs=max_epochs, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coyOEquwn6L1"
      },
      "source": [
        "### **Question 2.3**\n",
        "\n",
        "Now, we will implement the `LoRALinear` layer as a replacement for the standard `Linear` layer. This involves defining the following LoRA-specific parameters:\n",
        "- `low_rank_A`\n",
        "- `low_rank_B`\n",
        "- `rank`\n",
        "\n",
        "Additionally, we also includes the necessary code to facilitate training.\n",
        "\n",
        "**Question:**\n",
        "Explain your observations regarding the training behavior and performance on the new 'LoRAMLP'\n",
        "\n",
        "---\n",
        "\n",
        "**Answer your observations here:**\n",
        "\n",
        "ðŸ‘‰ **[YOUR ANSWER]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzKywNOKelJP"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LoRALinear(nn.Linear):\n",
        "    def __init__(self, in_features, out_features, rank, bias=True):\n",
        "        \"\"\"\n",
        "        LoRALinear layer with low-rank adaptation.\n",
        "\n",
        "        Args:\n",
        "            in_features (int): Number of input features.\n",
        "            out_features (int): Number of output features.\n",
        "            rank (int): Rank of the low-rank matrices for LoRA.\n",
        "            bias (bool): Whether to include a bias term.\n",
        "        \"\"\"\n",
        "        super().__init__(in_features, out_features, bias=bias)\n",
        "\n",
        "        # LoRA-specific parameters\n",
        "        ### Complete the following code of low_rank_A, low_rank_B and rank\n",
        "\n",
        "        # Initialize LoRA parameters\n",
        "        self.reset_lora_parameters()\n",
        "\n",
        "    def reset_lora_parameters(self):\n",
        "        \"\"\"Initialize LoRA-specific parameters.\"\"\"\n",
        "        nn.init.kaiming_uniform_(self.low_rank_A, a=5 ** 0.5)\n",
        "        nn.init.kaiming_uniform_(self.low_rank_B, a=5 ** 0.5)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Perform forward pass with LoRA adaptation.\n",
        "\n",
        "        Args:\n",
        "            input (Tensor): Input tensor of shape (batch_size, in_features).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Output tensor of shape (batch_size, out_features).\n",
        "        \"\"\"\n",
        "        ### Complete the following code of computing adapted weight: Original weight + low-rank update\n",
        "        return torch.nn.functional.linear(input, adapted_weight, self.bias)\n",
        "\n",
        "class LoRA_MLP(nn.Module):\n",
        "    def __init__(self, lora_rank):  # Default rank for LoRA\n",
        "        super().__init__()\n",
        "        ### Complete the following code of: LoRA_MLP\n",
        "        ### Use the created LoRALinear\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.seq(X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6f84URjos08"
      },
      "outputs": [],
      "source": [
        "def freeze_non_lora_parameters(model):\n",
        "    ### Complete the following code of: freezing none lora parameters,\n",
        "    ### consider also make the last linear layer trainable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSnHV0zAhYa5"
      },
      "outputs": [],
      "source": [
        "# use it\n",
        "lora_model = LoRA_MLP(lora_rank=8).to(device)  # You can adjust the rank as needed\n",
        "freeze_non_lora_parameters(lora_model)\n",
        "print_trainable_parameters(lora_model)\n",
        "##\n",
        "optimizer = torch.optim.Adam(lora_model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3lgXxwcgOFR"
      },
      "outputs": [],
      "source": [
        "train(lora_model, optimizer, criterion, train_dataloader, eval_dataloader, device=device, epochs=max_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpO8CdaHjIZA"
      },
      "source": [
        "## **Introduction to PEFT (Parameter-Efficient Fine-Tuning)**\n",
        "\n",
        "In this lab, weâ€™ve manually implemented LoRA for fine-tuning very simple MLP and Linear but this adapatation can be tedious to manage custom layers, weight transfers, and freezing parameters.\n",
        "\n",
        "Instead, we can use **Hugging Face's PEFT (Parameter-Efficient Fine-Tuning)** library. PEFT provides tools to easily integrate techniques like LoRA into your training pipeline without writing custom implementations. It automates tasks such as:\n",
        "- Adding low-rank layers to the model.\n",
        "- Freezing non-LoRA parameters.\n",
        "- Managing weight transfers and training only the LoRA-specific parameters.\n",
        "\n",
        "In the next part of this lab, we redo everything with peft library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkRwcBNBjVp9"
      },
      "outputs": [],
      "source": [
        "!pip install peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXeuB_YBr9iq"
      },
      "outputs": [],
      "source": [
        "import peft\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2zk5cwrrt5K"
      },
      "source": [
        "We list all names of the modules to decide which put into lora config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbaqA3vbjKhh"
      },
      "outputs": [],
      "source": [
        "[(n, type(m)) for n, m in MLP().named_modules()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOYVDxMRr8J8"
      },
      "source": [
        "Next we can define the LoRA config. There is nothing special going on here. We set the LoRA rank to 8 and select the layers `seq.0` and `seq.2` to be used for LoRA fine-tuning. As for `seq.4`, which is the output layer, we set it as `module_to_save`, which means it is also trained but no LoRA is applied."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiF8Xs_XjSJ0"
      },
      "outputs": [],
      "source": [
        "config = peft.LoraConfig(\n",
        "    r=2,\n",
        "    target_modules=[\"seq.0\", \"seq.2\"],\n",
        "    modules_to_save=[\"seq.4\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0osAx5_TlAR4"
      },
      "outputs": [],
      "source": [
        "module = MLP().to(device)\n",
        "peft_model = peft.get_peft_model(module, config)\n",
        "optimizer = ### Complete me\n",
        "criterion = ### Complete me"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u98xOcvOslTM"
      },
      "source": [
        "Now you can complete train the LoRA with `peft_model` directly, **complete the code.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHsX0xPTmfEl"
      },
      "outputs": [],
      "source": [
        "### Complete the following code:\n",
        "peft_model.print_trainable_parameters()\n",
        "train(...) ### Complete me"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dR0_XuiZtAOz"
      },
      "source": [
        "### **Using LoRA to fine-tune CLIP**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW6sJHenvR_T"
      },
      "source": [
        "### Reuse dataset Food101\n",
        "Implement the dataset class for UPMC-Food101 dataset.\n",
        "For this, you will need to:\n",
        "- Download the data [here](https://drive.google.com/file/d/1KT9Agi0rbGb8x8hRLl6ED1cCqCcKtBkS/view?usp=sharing)\n",
        "- Split it in train, val, test sets using the following ratios: 0.7, 0.1, 0.2.\n",
        "- Create a dataset class inherited from `torch.utils.data.Dataset` that loads the data, and preprocesses it.\n",
        "\n",
        "Tips:\n",
        "1. In `PyTorch`, the dataset class should inherint from [`torch.utils.data.Dataset`](https://pytorch.org/tutorials/beginner/data_loading_tutorial#dataset-class) and always override:\n",
        "- `__len__`: so that `len(dataset)` returns the size of the dataset.\n",
        "- `__getitem__`: to support the indexing such that `dataset[i]` can be used to get the `i`-th sample.\n",
        "\n",
        "\n",
        "2. If you use colab, to ease the access to the data, you can mount your google drive and access the data from there.\n",
        "- *Mount your Google Drive* to allow Colab to access files in your Drive.\n",
        "```python\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "```\n",
        "- Access the [file](https://drive.google.com/file/d/1KT9Agi0rbGb8x8hRLl6ED1cCqCcKtBkS/view?usp=sharing) with the data, this will automatically add it to your 'shared folder'\n",
        "- Locate the Shared Folder which is typically stored under My Drive > Shared with me. (However, only items in \"My Drive\" or folders youâ€™ve added to your Drive can be accessed directly.)\n",
        "- Add the Shared Folder to 'My Drive'. Drag & drop the folder in 'My Drive' folder.\n",
        "- The folder with the data should now be in My Drive/LabeledDB_new\n",
        "- It should now be located in the path '/content/drive/My Drive/food101.arrow'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dEnYku_6ga8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zApWbwHvR_V"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = # TODO: Change this to your data path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3W-1fhj5HQl"
      },
      "outputs": [],
      "source": [
        "!pip install peft\n",
        "!pip install pyarrow\n",
        "\n",
        "import os\n",
        "import io\n",
        "import pyarrow as pa\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from timm.data.constants import (\n",
        "    IMAGENET_DEFAULT_MEAN,\n",
        "    IMAGENET_DEFAULT_STD,\n",
        ")\n",
        "\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UU2olHDcvR_V"
      },
      "outputs": [],
      "source": [
        "def open_arrow(data_dir, name):\n",
        "    path = os.path.join(data_dir, f\"{name}.arrow\")\n",
        "    assert os.path.isfile(path)\n",
        "    table = pa.ipc.RecordBatchFileReader(\n",
        "        pa.memory_map(path, \"r\")\n",
        "    ).read_all()\n",
        "    all_texts = table[\"text\"].to_pandas().to_list()\n",
        "    all_texts = [text[0] for text in all_texts]\n",
        "    all_images = table[\"image\"].to_pandas().to_list()\n",
        "    all_labels = table[\"label\"].to_pandas().to_list()\n",
        "    return all_texts, all_images, all_labels\n",
        "CLASSES_FOOD101 = [\n",
        "        \"frozen yogurt\", \"tacos\",\"gnocchi\", \"ramen\", \"sushi\", \"spaghetti carbonara\", \"foie gras\", \"club sandwich\",\n",
        "        \"chicken curry\", \"caprese salad\", \"guacamole\", \"fish and chips\", \"seaweed salad\", \"samosa\", \"shrimp and grits\",\n",
        "        \"prime rib\", \"cheese plate\", \"grilled cheese sandwich\", \"omelette\", \"macarons\", \"pancakes\", \"paella\",\n",
        "        \"crab cakes\", \"onion rings\", \"poutine\", \"panna cotta\", \"hamburger\", \"eggs benedict\", \"pizza\", \"cup cakes\",\n",
        "        \"red velvet cake\", \"huevos rancheros\", \"french toast\", \"pad thai\",\"beef tartare\", \"beet salad\",\n",
        "        \"garlic bread\", \"escargots\", \"cheesecake\", \"tuna tartare\", \"french fries\", \"pulled pork sandwich\", \"ravioli\",\n",
        "        \"clam chowder\", \"baby back ribs\", \"pork chop\", \"chicken quesadilla\", \"grilled salmon\", \"dumplings\",\n",
        "        \"fried rice\", \"macaroni and cheese\", \"hot and sour soup\", \"tiramisu\",  \"lobster_roll_sandwich\",\n",
        "        \"filet mignon\",  \"lobster bisque\",  \"risotto\",  \"takoyaki\",  \"waffles\",  \"miso soup\",  \"scallops\",\n",
        "        \"hummus\", \"french onion soup\", \"spaghetti bolognese\", \"gyoza\", \"sashimi\", \"caesar salad\",\n",
        "        \"donuts\", \"pho\", \"creme brulee\", \"edamame\",\"apple pie\", \"carrot cake\", \"croque madame\",\n",
        "        \"chocolate_cake\", \"mussels\", \"chocolate mousse\", \"deviled eggs\", \"ice cream\",\n",
        "        \"chicken_wings\", \"steak\", \"churros\", \"beignets\", \"breakfast burrito\", \"spring rolls\",\n",
        "        \"lasagna\", \"cannoli\", \"nachos\", \"bruschetta\", \"hot dog\", \"greek salad\", \"beef carpaccio\",\n",
        "        \"falafel\", \"strawberry shortcake\", \"bread pudding\", \"ceviche\", \"peking duck\",\n",
        "        \"bibimbap\", \"oysters\", \"fried calamari\", \"baklava\"\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use directly the code from last week."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvIX0VdyvR_V"
      },
      "outputs": [],
      "source": [
        "class MultimodalDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Food101 dataset.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        path: str,\n",
        "        name: str, \n",
        "        split: str = None,\n",
        "        img_transforms = None,\n",
        "    ) -> None:\n",
        "        \"\"\"Initialize Food101 dataset.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        path : str\n",
        "            Path where the dataset is stored.\n",
        "        name: str\n",
        "            Name of the file to load.\n",
        "        split : str, optional\n",
        "            Split to use, by default None.\n",
        "        img_transforms : torchvision.transforms, optional\n",
        "            Image transformations, by default None.\n",
        "        split : str, optional\n",
        "            split to use, e.g. \"train\", \"val\", \"test\".\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # To complete\n",
        "        pass\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Len of the dataset.\"\"\"\n",
        "        # To complete\n",
        "        pass\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Get one item from the dataset of the given index.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        index : int\n",
        "            Index of the item to get.\n",
        "        \"\"\"\n",
        "        # To complete\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flFPRJX_wEYQ"
      },
      "source": [
        "Make sure the dataset works fine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYKfkxZ1vbfL"
      },
      "outputs": [],
      "source": [
        "im_transforms = torchvision.transforms.Compose(\n",
        "    [\n",
        "        torchvision.transforms.Resize((224, 224)),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "    ])\n",
        "idx = 2\n",
        "dataset = MultimodalDataset(path=DATA_DIR,\n",
        "                            name=\"food101\",\n",
        "                            split=\"test\",\n",
        "                            img_transforms=im_transforms,)\n",
        "# - display one image\n",
        "image, text_value, label  = dataset[idx]\n",
        "plt.imshow(image.numpy().transpose(1, 2, 0))\n",
        "plt.suptitle(f'Label: {CLASSES_FOOD101[label]}')\n",
        "plt.title(text_value, fontsize=8)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddt5qeCawUN-"
      },
      "source": [
        "Again we evaluate the zero-shot performance of the CLIP model on the UPMC-Food101 dataset. Use `clip.tokenize(text_labels)` to tokenize the text labels and `clip.load(\"ViT-B/32\", device=device)[0]` to load the CLIP pretrained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seS2DkSUu_GJ"
      },
      "outputs": [],
      "source": [
        "img_transforms = torchvision.transforms.Compose(\n",
        "    [\n",
        "        torchvision.transforms.Resize((224, 224)),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize(\n",
        "            IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
        "        ),\n",
        "    ])\n",
        "\n",
        "test_set_food = MultimodalDataset(path=DATA_DIR,\n",
        "                            name=\"food101\",\n",
        "                            split=\"test\",\n",
        "                            img_transforms=img_transforms)\n",
        "test_loader_food = torch.utils.data.DataLoader(test_set_food, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6WHsbXTu_GJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate_clip_multimodal_hf(model, processor, test_loader, classes, prefix, device):\n",
        "    \"\"\"\n",
        "    Evaluate a CLIP model (Hugging Face) on a classification task.\n",
        "\n",
        "    Args:\n",
        "        model (CLIPModel): A Hugging Face CLIP model.\n",
        "        processor (CLIPProcessor): A Hugging Face CLIP processor for tokenizing and image preprocessing.\n",
        "        test_loader (DataLoader): DataLoader for the test set, yielding (images, _, labels).\n",
        "        classes (list): List of class labels.\n",
        "        prefix (str): Text prefix for prompt engineering (e.g., \"A photo of a\").\n",
        "        device (torch.device): The device (CPU or GPU) to perform evaluation on.\n",
        "    \"\"\"\n",
        "    # Prepare text prompts (labels)\n",
        "    text_labels = [f\"{prefix}{c}\" for c in classes]  # e.g., \"A photo of a dog\"\n",
        "\n",
        "    # Tokenize text labels with HF processor\n",
        "    text_inputs = processor(text=text_labels, return_tensors=\"pt\", padding=True).to(device)\n",
        "\n",
        "    test_acc = 0.0\n",
        "    all_labels, all_predictions = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader):\n",
        "            images, _, labels = batch\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Preprocess images\n",
        "            image_inputs = processor(images=images, return_tensors=\"pt\",\n",
        "                                     do_normalize=False, do_resize=False, do_rescale=False,).to(device)\n",
        "\n",
        "            outputs = model(**image_inputs, **text_inputs)\n",
        "            logits_per_image = outputs.logits_per_image  # (batch_size, num_classes)\n",
        "\n",
        "            probs = logits_per_image.softmax(dim=-1).cpu()\n",
        "            predicted = probs.argmax(dim=-1)\n",
        "\n",
        "            test_acc += (predicted == labels.cpu()).sum().item()\n",
        "            all_labels.append(labels.cpu())\n",
        "            all_predictions.append(predicted)\n",
        "\n",
        "    test_acc /= len(test_loader.dataset)\n",
        "    print(f\"Test Acc: {test_acc:.3f}\")\n",
        "\n",
        "# Initialize the Hugging Face CLIP model and processor\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_clip_multimodal_hf(\n",
        "    model=model,\n",
        "    processor=processor,\n",
        "    test_loader=test_loader_food,\n",
        "    classes=CLASSES_FOOD101,\n",
        "    prefix=\"\",\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fxo0ETvLzG5Z"
      },
      "source": [
        "### Question 2.4:\n",
        "Now we use the peft library to fine-tune our clip more efficiently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7DLECqHz1HR"
      },
      "outputs": [],
      "source": [
        "batch_size=128\n",
        "im_transforms_train = torchvision.transforms.Compose(\n",
        "    [transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Crop but keep most of food visible\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.RandomRotation(15),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(\n",
        "        IMAGENET_DEFAULT_MEAN,\n",
        "        IMAGENET_DEFAULT_STD\n",
        "    ), ]\n",
        "    )\n",
        "im_transforms = torchvision.transforms.Compose(\n",
        "[\n",
        "    torchvision.transforms.Resize((224, 224)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(\n",
        "        IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
        "    ),\n",
        "])\n",
        "dataset = MultimodalDataset(path=DATA_DIR,\n",
        "                            name=\"food101\",\n",
        "                            split=\"train\",\n",
        "                            img_transforms=im_transforms_train)\n",
        "test_dataset = MultimodalDataset(path=DATA_DIR,\n",
        "                        name=\"food101\",\n",
        "                        split=\"test\",\n",
        "                        img_transforms=im_transforms)\n",
        "val_dataset = MultimodalDataset(path=DATA_DIR,\n",
        "                                name=\"food101\",\n",
        "                                split=\"val\",\n",
        "                                img_transforms=im_transforms)\n",
        "dataloader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                        batch_size=batch_size,\n",
        "                                        shuffle=True,\n",
        "                                        drop_last=False)\n",
        "val_dataloader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
        "                                            batch_size=batch_size,\n",
        "                                            shuffle=False,\n",
        "                                            drop_last=False)\n",
        "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=False,\n",
        "                                              drop_last=False)\n",
        "optimizer = torch.optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=1e-3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFLMIsqc0I_a"
      },
      "outputs": [],
      "source": [
        "def train_clip_lora(\n",
        "    model,\n",
        "    processor,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    classes,\n",
        "    epochs=5,\n",
        "    lr=1e-3,\n",
        "    prefix=\"\",\n",
        "):\n",
        "    optimizer = torch.optim.Adam(\n",
        "        filter(lambda p: p.requires_grad, model.parameters()),\n",
        "        lr=lr\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0.0\n",
        "\n",
        "        for images, texts, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\"):\n",
        "            ### Complete the training and the validataion\n",
        "        print(f\"[Epoch {epoch+1}] Validation Loss: {avg_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEYtfeRm0iH4"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "# device=\"cpu\"\n",
        "# Example LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Adjust to match your model's layer names\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        ")\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "\n",
        "lora_model = get_peft_model(model, lora_config)\n",
        "print_trainable_parameters(lora_model)\n",
        "\n",
        "print(len(CLASSES_FOOD101))\n",
        "\n",
        "Fine-tune with LoRA\n",
        "train_clip_lora(\n",
        "    model=lora_model,\n",
        "    processor=processor,\n",
        "    train_loader=dataloader,\n",
        "    val_loader=val_dataloader,\n",
        "    classes=CLASSES_FOOD101,\n",
        "    epochs=5,\n",
        "    lr=1e-3,\n",
        "    prefix=\"\"  # example prompt, try also the prompting if you want\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9jKbX6D27-j"
      },
      "source": [
        "Evaluate your lora-model to see if the lora helps the tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zw0xb3VW2bjh"
      },
      "outputs": [],
      "source": [
        "evaluate_clip_multimodal_hf(\n",
        "    model=lora_model,\n",
        "    processor=processor,\n",
        "    test_loader=test_loader_food,\n",
        "    classes=CLASSES_FOOD101,\n",
        "    prefix=\"\",\n",
        "    device=device\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
